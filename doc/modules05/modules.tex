%
% File draft.tex
%

\documentclass[11pt]{article}
\usepackage{acl2005}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\newcommand{\hpsg}{\textsc{hpsg}}
\newcommand{\lkb}{\textsc{lkb}}
\newcommand{\lfg}{\textsc{lfg}}

\title{Rapid Prototyping of Scalable Grammars:\\
       Modular Extensions to a Language-Independent Core}
%% \author{Emily M.~Bender\\
%% Department of Linguistics\\
%% University of Washington\\
%% Box 354340\\
%% Seattle, WA 98195-4340, USA\\
%% {\small {\tt ebender@u.washington.edu}}
%% \And
%% Dan Flickinger\\
%% CSLI, Stanford University\\
%% Ventura Hall, 220 Panama St\\
%% Stanford, CA 94305, USA\\
%% {\small {\tt danf@csli.stanford.edu}}
%% }

\author{anonymous}
\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper presents a novel approach to simpliyfing the construction of 
precise broad-coverage grammars, employing typologically-motivated modules
as customizable extensions to a language-independent core grammar.  Each
module represents some salient dimension of cross-linguistic variation such
as word order or negation, and presents the grammar developer with simple 
choices that result in automatically generated language-specific software 
that becomes part of the grammar implementation for that language.  The 
paper illustrates this notion of a module for several phenomena, and 
includes an evaluation of the approach against multilingual test data
reflecting interactions among these phenomena.
\end{abstract}

\section{Introduction}
Manual development of precise broad-coverage grammar implementations
is a labor-intensive undertaking, typically requiring multiple years
of work by highly trained computational linguists.  Recent efforts
toward reducing the time and level of expertise needed for producing a
new grammar have focused either on adapting an existing grammar of a
related language \cite{Butt-et-al-02,Kim:Dal:Kap:Kin:Mas:Ohk:03}, or
on identifying a set of language-independent grammar constraints to
which language-specific constraints can be added~\cite{Ben:Fli:Oe:02}.
Neither implementation approach has benefited from the substantial
theoretical work on language typology, which characterizes linguistic
phenomena and the varied mechanisms that languages employ to realize
them.  In this paper we report on an elaboration of the Grammar Matrix
approach in which phenomenon-specific modules encode the dimensions of
variation, and present the grammar developer with simple choices that
determine this language's type for each phenomenon. Each module then
automatically generates the necessary language-specific constraints as
extensions to the Matrix grammar files, enabling the linguist to
produce a working prototype grammar with very little effort or
computational expertise.  We illustrate this concept of a
typology-based module for several phenomena including basic word
order, sentence negation, and yes-no questions, and we conclude with
an evaluation of the resulting grammar implementations for a variety
of languages.

\section{The Grammar Matrix}

The past decade has seen the development of wide-coverage implemented
grammars representing deep linguistic analysis of several languages in
several frameworks, including Head-Driven Phrase Structure Grammar
(\hpsg), Lexical-Functional Grammar (\lfg), and Lexicalized Tree
Adjoining Grammar ({\sc ltag}). In \hpsg\ \cite{Pol:Sag:94}, the most
extensive grammars are those of English \cite{Flickinger:00}, German
\cite{Mue:Kap:00}, and Japanese \cite{Siegel:00,Siegel:Bender:02}.
The Grammar Matrix \cite{Ben:Fli:Oe:02} is an attempt to distill the
wisdom of these existing grammars and document it in a form that can
be used as the basis for new grammars. The main goals of the project
are: (i) consistent with other work in \hpsg, to develop in detail
semantic representations and in particular the syntax-semantics
interface; (ii) to represent generalizations across linguistic objects
and across languages; and (iii) through the richness of the matrix and
its built-in links with the \lkb\ grammar engineering environment
\cite{Copestake:02}, to allow for extremely quick start-up as the
matrix is applied to new languages.

The first, preliminary version of the Grammar Matrix 
consisted of types defining the basic feature geometry and technical
devices (e.g., for list manipulation), types associated with Minimal
Recursion Semantics (see, e.g., \cite{Cop:Las:Fli:01}) types for
general classes of rules (including derivational and inflectional
lexical rules, unary and binary phrase structure rules, headed and
non-headed rules, and head-initial and head-final rules), and types
for basic constructions such as head-complement, head-specifier,
head-subject, head-filler, and head-modifier rules, as well as 
coordination and more specialized classes of constructions, such as relative
clauses and noun-noun compounding.  In addition, the preliminary
version of the Grammar Matrix included configuration and parameter
files for the \lkb.  The current released version of the Matrix further 
includes a hierarchy of lexical types for creating language-specific 
lexical entries.

All of the constraints in the current Matrix are intended to be 
language-independent, and monotonically extensible when developing a
new grammar for a language, though not all types defined in the Matrix
will necessarily be employed for any given language.  This strong 
assumption of universality has been of practical utility in the early
development of the matrix, but it has sharply limited the inventory of
constraint definitions that could be supplied to grammar developers
since many generalizations hold only for subsets of languages.  It is
this limitation of the current Matrix that we address by introducing
the notion of typology-based modules.

\section{Typology-based modules}

In general, we find two kinds of typological variation across
languages.  On the one hand, there are systems (formal or functional)
which must be represented in every language.  Word order is a prime
example of a formal system represented in every language: Every
language has some set of permissible word orders.  Those sets differ
from language to language, but there are recurring patterns.
A functional system which is represented in every language is
sentential negation.  To our knowledge, all human languages provide
some means of expressing the negation of a statement.  There are many
strategies for this, and most, if not all of those strategies, appear
in more than one language.  On the other hand, there are linguistic
phenomena which appear in only some languages, and are difficult to
conceptualize as alternative realizations of the same function.  More
importantly, for our purposes, ordinary working linguists (`OWLs')
don't conceptualize them in this way.  These include things like
noun incorporation, numeral classifiers, and auxiliary verbs.
Not all languages instantiate these phenomena, but among those that
do, one finds recurring varieties which can be subjected to typological
analysis (see, for example, \cite{Mithun:84} on noun incorpration).
The type of modular extension to the Matrix which we propose here
is meant to scale up to handle both kinds of typological variation.

The core Matrix and modular extensions to it may appear at first
glance to be analogous to the Principles and Parameters proposed
in mainstream syntactic theory, beginning with \newcite{Chomsky81}.
However, there is at least one important difference between the
approaches.  Whereas Parameters are meant to be abstract `switches'
which simultaneously control multiple different, apparently 
unrelated surface phenomena, the modules in the Matrix each directly
encode the constraints necessary to handle one particular phenomenon.
Nonetheless, this does not make the modules themselves trivial:
the modules need to be carefully designed in order to be mutually
consistent, ideally across all possible combinations.  

We would like to stress that our goals with this project include a
bottom-up, data-driven investigation of linguistic universals and
constraints on cross-linguistic variation.  Thus in building modules
for any particular function or phonemenon, we work from the assumption
that we have not yet found an exhaustive classification of languages
along that dimension.  This is certainly true of the implemented
modules presented below.  Furthermore, we are cautious about making
theoretical claims based on interactions between the modules.  If we
had, for example, a word-order alternative which was incompatible with
a noun incorporation alternative, this might be interpreted as a
prediction that that particular combination is never found in the
world's languages.  This would almost certainly be
over-interpretation, however, and it seems the better course to strive
for compatibility and leave to long-term typological investigation the
discovery of patterns of co-occurrence.  A system such as the one we
propose could be instrumental in assisting large scale typological
investigations (covering hundreds of languages) in studying deeper,
more subtle facts about languages.

Consistent with earlier versions of the Matrix, we aim to support
both rapid precision grammar prototyping and scalable solutions.
That is, the grammar prototype created automatically by the system
described here should be one that a linguist could then work with
to incrementally improve.  This sets a high bar for the modules themselves,
requiring them to be not only first-pass solutions, but actually
good first approximations which may need to be refined but not thrown out.
It also requires that the automatically generated grammar files 
maintain a high degree of readability so that they may be effectively
modified.  Similarly, there are issues in updating procedures: if
a linguist is working with a Matrix-generated grammar, how best to
propogate updates in the Matrix, despite potential changes to automatically
generated files.  Finally, the UI for the prototyping stage must allow
the linguist to go back and revise decisions in the face of new
information or revised linguistic analyses.  This again raises the 
issue of how to best achieve automated revisions in the face of hand-editing
of autogenerated files.  We'll return to these issues in \S\ref{management}
below.

[IS IT WORTH MAKING A LIST OF THINGS WHICH MIGHT BE COVERED IN
THIS WAY, OR IS THE GENERAL DISCUSSION AT THE BEGINNING OF THIS SECTION
DETAILED ENOUGH?]

[SAY SOMETHING ABOUT EVENTUALLY SUPPORING A WAY FOR THE MATRIX USER
COMMUNITY TO PROPOSE ADDITIONAL MODULES?  THAT'S ALL FUTURE WORK, OF
COURSE.  ANYTHING ELSE ABOUT REUSE OF ANALYSES?]


\section{Implementations of three modules}

To explore the issues that will arise both in developing modules which
are consistent with each other and in managing the modules, we have
implemented an initial set of modules targeting three areas: basic
word order, matrix yes-no questions, and sentential negation.  The coverage
of these modules and the strategies taken in their implementation are
described in the following subsections.

\subsection{Word order}

The word order modules address the so-called basic word order in
a language: the relative order of subjects, verbs, and verbal complements.
Languages vary in their ridigity with respect to word order and the
question of how to determine the basic word-order of a language is 
notoriously complex.  Nonetheless, we believe that many if not most
linguists working on linguistic description would analyze some orders
as primary and others as derived.  Thus the basic word order modules
are meant to capture the relative ordering of subjects, verbs, and complements
in clauses which don't also involve word-order changing phenomena
such as topicalization, extraposition, subject-verb (or subject-auxiliary)
inversion, etc.  Eventual modules for such phenomena would need to
interact appropriately with the basic word-order modules.

The Grammar Matrix core grammar provides definitions of basic
head-complement and head-subject schemata which are consistent with
the Matrix implementation of compositional semantics
\cite{Fli:Ben:03}, as well as definitions of head-initial and
head-final phrase types.  These rules are also consistent with the
HPSG-style handling of long-distance dependencies implemented in the
Matrix.\footnote{We note that the long-distance dependency machinery
might be best treated as a module itself rather than part of the core
grammar.}  With this infrastructure, the implementation of the basic
word order modules was mostly a matter of creating subtypes joining
head-complement and head-subject schemata with the right types
specifying head/dependent order, in addition to creating instances of
those types as required by the \lkb\ parser.

The word order modules implemented so far handle the following
possibilities SOV, SVO, VSO, VOS, OVS, OSV, V-final, V-initial, and
free word order.\footnote{VOS, OVS and OSV are extremely rare among
the world's languages, with OSV being arguably unattested.
Nonetheless, we decided to implement support for all of them.}  In the
strict word orders, only one head-complement and one head-subject rule
are posited, with a strict ordering between them.  For V-final and
V-initial order (where the order of subjects and complements are
unconstrained), again we have only two rules, but they are allowed to
apply in either order.  For the free word order case, we posited both
head-initial and head-final head-complement and head-subject rules.
This alone is enough to capture the 6 possible word orders, but
without further work it leads to considerable spurious ambiguity in
the cases where there are arguments on both sides of the verb.  To
solve this problem, we adapted a strategy from the English Resource
Grammar for requiring attachment to the right before attachment to the
left.\footnote{We believe that this approach is linguistically
appropriate as long as the languages in question cannot be said to
have a VP constituent in OVS clauses.  Matrix grammars produce
maximally binary-branching structures.  This is convenient in that it
reduces the number of rules a grammar needs to encode, though at a
cost of arguably more complex trees.  We maintain that not all of the
constituents represented in such a tree need to be interpreted as
linguistically meaningful.  Conversely, however, any such grammar
should posit a constituent where the linguistic evidence says there
must be one.}

Figure~\ref{wordorderfig} shows the types files for the SOV and
V-final orders and instance file which they both share.

The core Matrix also provides a lexical type hierarchy
providing basic types for (among other things) nouns and verbs which
can be cross-classified with a set of types for various argument
structure patterns.  For the purposes of testing these modules,
we made a set of lexical types inheriting from these dimensions and
instantiated by a test lexicon.  In addition, to finesse the issue
of NP structure while still producing parses with well-formed semantic
representations (for testing via generation), we posited a place-holder
module providing an underconstrained determinerless NP rule.

-- basic S [comp] V order

-- strategy for V-initial or V-final; reuse and another sense of ``modularity''.
-- strategy for ``free'' word order

-- not handled yet: mixed word order (by head type, by clause type); pragmatic effects of word order in ``free'' word order languages

-- reorderings among complements: somewhat orthogonal. implemented so far just for ``free''.  [add for V-initial and V-final?]


\subsection{Yes-no questions}

\subsection{Sentential negation}

\section{Management of modules}
\label{management}

-- perl scripts/form idea: ideal case has an interactive interface
where decisions lead to more choices to be made, linguist can see
the decisions taken at a glance, 

-- could conceivably just have lots and lots of different files, but:
missed generalizations, need to input spellings of things

-- current solution is a perl script, for testing purposes

\section{Evaluation}

-- Table of languages and properties

-- Describe test suites: admit that the sample is entirely opportunistic,
and that the point of the evaluation is to illustrate the usefulness of the
technique across and interesting range of languages.  not a systematic 
typological survey.

-- Cite sources

-- Describe coverage, things about languages that are not covered

-- Describe the amount of work it takes to make a prototype with the
perl script, and compare to what it would take to do it from scratch

\section{Conclusion and outlook}

-- linguistic/scientific interest of modules: comparative analyses
across languages

-- ref to Montage? (or maybe save that for final version, when 
we're not trying to be anonymous)

-- mention uses of prototyping, for documentary linguistics, pedagogical
purposes, industrial-strength grammar engineering (including assisting
minority languages in producing resources relatively cheeply), and perhaps
benefits to machine-learning approaches -- can we imagine any?  something
along the lines of presenting typological properties in a machine-readable
way to improve ML algorithms which might need to be tuned differently for
different kinds of languages

-- Modularity in grammar engineering: observations about how what looks
like a module from a typological perspective is integrated with the rest
of the grammar from the perspective of any given language. Reiterate about
needing to be careful about cross-module consistency. Also :+.

-- Modularity in grammar engineering: reuse of bits across modules within
a system.  word order rule reuse, perl-script based specialization of 


\section*{Acknowledgements}

%\section*{References}

\bibliographystyle{acl}
%\bibliographystyle{robbib}
\bibliography{modules}


\end{document}
