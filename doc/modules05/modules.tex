%
% File draft.tex
%

\documentclass[11pt]{article}
\usepackage{acl2005}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\newcommand{\hpsg}{\textsc{hpsg}}
\newcommand{\lkb}{\textsc{lkb}}
\newcommand{\lfg}{\textsc{lfg}}

\title{Rapid Prototyping of Scalable Grammars:\\
       Modular Extensions to a Language-Independent Core}
%% \author{Emily M.~Bender\\
%% Department of Linguistics\\
%% University of Washington\\
%% Box 354340\\
%% Seattle, WA 98195-4340, USA\\
%% {\small {\tt ebender@u.washington.edu}}
%% \And
%% Dan Flickinger\\
%% CSLI, Stanford University\\
%% Ventura Hall, 220 Panama St\\
%% Stanford, CA 94305, USA\\
%% {\small {\tt danf@csli.stanford.edu}}
%% }

\author{anonymous}
\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper presents a novel approach to simplifying the construction of 
precise broad-coverage grammars, employing typologically-motivated modules
as customizable extensions to a language-independent core grammar.  Each
module represents some salient dimension of cross-linguistic variation such
as word order or negation, and presents the grammar developer with simple 
choices that result in automatically generated language-specific software 
that becomes part of the grammar implementation for that language.  The 
paper illustrates this notion of a module for several phenomena, and 
includes an evaluation of the approach against multilingual test data
reflecting interactions among these phenomena.
\end{abstract}

\section{Introduction}
Manual development of precise broad-coverage grammar implementations
is a labor-intensive undertaking, typically requiring multiple years
of work by highly trained computational linguists.  Recent efforts
toward reducing the time and level of expertise needed for producing a
new grammar have focused either on adapting an existing grammar of a
related language \cite{Butt-et-al-02,Kim:Dal:Kap:Kin:Mas:Ohk:03}, or
on identifying a set of language-independent grammar constraints to
which language-specific constraints can be added~\cite{Ben:Fli:Oe:02}.
Neither implementation approach has benefited from the substantial
theoretical work on language typology, which characterizes linguistic
phenomena and the varied mechanisms that languages employ to realize
them.  In this paper we report on an elaboration of the Grammar Matrix
approach in which phenomenon-specific modules encode the dimensions of
variation, and present the grammar developer with simple choices that
determine this language's type for each phenomenon. Each module then
automatically generates the necessary language-specific constraints as
extensions to the Matrix grammar files, enabling the linguist to
produce a working prototype grammar with very little effort or
computational expertise.  We illustrate this concept of a
typology-based module for several phenomena including basic word
order, sentence negation, and yes-no questions, and we conclude with
an evaluation of the resulting grammar implementations for a variety
of languages.

\section{The Grammar Matrix}

The past decade has seen the development of wide-coverage implemented
grammars representing deep linguistic analysis of several languages in
several frameworks, including Head-Driven Phrase Structure Grammar
(\hpsg), Lexical-Functional Grammar (\lfg), and Lexicalized Tree
Adjoining Grammar ({\sc ltag}). In \hpsg\ \cite{Pol:Sag:94}, the most
extensive grammars are those of English \cite{Flickinger:00}, German
\cite{Mue:Kap:00}, and Japanese \cite{Siegel:00,Siegel:Bender:02}.
The Grammar Matrix \cite{Ben:Fli:Oe:02} is an attempt to distill the
wisdom of these existing grammars and document it in a form that can
be used as the basis for new grammars. The main goals of the project
are: (i) consistent with other work in \hpsg, to develop in detail
semantic representations and in particular the syntax-semantics
interface; (ii) to represent generalizations across linguistic objects
and across languages; and (iii) through the richness of the matrix and
its built-in links with the \lkb\ grammar engineering environment
\cite{Copestake:02}, to allow for extremely quick start-up as the
matrix is applied to new languages.

The first, preliminary version of the Grammar Matrix 
consisted of types defining the basic feature geometry and technical
devices (e.g., for list manipulation), types associated with Minimal
Recursion Semantics (see, e.g., \cite{Cop:Las:Fli:01}) types for
general classes of rules (including derivational and inflectional
lexical rules, unary and binary phrase structure rules, headed and
non-headed rules, and head-initial and head-final rules), and types
for basic constructions such as head-complement, head-specifier,
head-subject, head-filler, and head-modifier rules, as well as 
coordination and more specialized classes of constructions, such as relative
clauses and noun-noun compounding.  In addition, the preliminary
version of the Grammar Matrix included configuration and parameter
files for the \lkb.  The current released version of the Matrix further 
includes a hierarchy of lexical types for creating language-specific 
lexical entries.

All of the constraints in the current Matrix are intended to be 
language-independent, and monotonically extensible when developing a
new grammar for a language, though not all types defined in the Matrix
will necessarily be employed for any given language.  This strong 
assumption of universality has been of practical utility in the early
development of the matrix, but it has sharply limited the inventory of
constraint definitions that could be supplied to grammar developers
since many generalizations hold only for subsets of languages.  It is
this limitation of the current Matrix that we address by introducing
the notion of typology-based modules.

\section{Typology-based modules}

In general, we find two kinds of typological variation across
languages.  On the one hand, there are systems (formal or functional)
which must be represented in every language.  Word order is a prime
example of a formal system represented in every language: Every
language has some set of permissible word orders.  Those sets differ
from language to language, but there are recurring patterns.
A functional system which is represented in every language is
sentential negation.  To our knowledge, all human languages provide
some means of expressing the negation of a statement.  There are many
strategies for this, and most, if not all of those strategies, appear
in more than one language.  On the other hand, there are linguistic
phenomena which appear in only some languages, and are difficult to
conceptualize as alternative realizations of the same function.  More
importantly, for our purposes, ordinary working linguists (`OWLs')
don't conceptualize them in this way.  These include things like
noun incorporation, numeral classifiers, and auxiliary verbs.
Not all languages instantiate these phenomena, but among those that
do, one finds recurring varieties which can be subjected to typological
analysis (see, for example, \cite{Mithun:84} on noun incorporation).
The type of modular extension to the Matrix which we propose here
is meant to scale up to handle both kinds of typological variation.

The core Matrix and modular extensions to it may appear at first
glance to be analogous to the Principles and Parameters proposed
by \newcite{Chomsky81} and others.
However, they differ in at least one important respect:
Whereas Parameters are meant to be abstract `switches'
which simultaneously control multiple different, apparently 
unrelated surface phenomena, the modules in the Matrix each directly
encode the constraints necessary to handle one particular phenomenon.
Nonetheless, this does not make the modules themselves trivial:
the modules need to be carefully designed in order to be mutually
consistent, ideally across all possible combinations.  

We would like to stress that our goals with this project include a
bottom-up, data-driven investigation of linguistic universals and
constraints on cross-linguistic variation.  Thus in building modules
for any particular function or phenomenon, we work from the assumption
that we have not yet found an exhaustive classification of languages
along that dimension.  This is certainly true of the implemented
modules presented below.  Furthermore, we are cautious about making
theoretical claims based on interactions between the modules.  If we
had, for example, a word-order alternative which was incompatible with
a noun incorporation alternative, this might be interpreted as a
prediction that that particular combination is never found in the
world's languages.  This would almost certainly be
over-interpretation, however, and it seems the better course to strive
for compatibility and leave to long-term typological investigation the
discovery of patterns of co-occurrence.  A system such as the one we
propose could be instrumental in assisting large scale typological
investigations (covering hundreds of languages) in studying deeper,
more subtle facts about languages.

Consistent with earlier versions of the Matrix, we aim to support
both rapid precision grammar prototyping and scalable solutions.
That is, the grammar prototype created automatically by the system
described here should be one that a linguist could then work with
to incrementally improve.  This sets a high bar for the modules themselves,
requiring them to be not only first-pass solutions, but actually
good first approximations which may need to be refined but not thrown out.
It also requires that the automatically generated grammar files 
maintain a high degree of readability so that they may be effectively
modified.  Similarly, there are issues in updating procedures: if
a linguist is working with a Matrix-generated grammar, how best to
propagate updates in the Matrix, despite potential changes to automatically
generated files.  Finally, the UI for the prototyping stage must allow
the linguist to go back and revise decisions in the face of new
information or revised linguistic analyses.  This again raises the 
issue of how to best achieve automated revisions in the face of hand-editing
of autogenerated files.  We'll return to these issues in \S\ref{management}
below.

[IS IT WORTH MAKING A LIST OF THINGS WHICH MIGHT BE COVERED IN
THIS WAY, OR IS THE GENERAL DISCUSSION AT THE BEGINNING OF THIS SECTION
DETAILED ENOUGH?]

[SAY SOMETHING ABOUT EVENTUALLY SUPPORTING A WAY FOR THE MATRIX USER
COMMUNITY TO PROPOSE ADDITIONAL MODULES?  THAT'S ALL FUTURE WORK, OF
COURSE.  ANYTHING ELSE ABOUT REUSE OF ANALYSES?]


\section{Implementations of three modules}

To explore the issues that will arise in developing modules which
are consistent with each other and in module management, we have
implemented an initial set of modules targeting basic
word order, matrix yes-no questions, and sentential negation.  
%The coverage
%of these modules and the strategies taken in their implementation are
%described in the following subsections.

\subsection{Word order}

The word order modules address the so-called basic word order in
a language: the relative order of subjects, verbs, and verbal complements.
Languages vary in their rigidity with respect to word order and the
question of how to determine the basic word-order of a language is 
notoriously complex.  Nonetheless, we believe that many if not most
linguists working on linguistic description would analyze some orders
as primary and others as derived.  Thus the basic word order modules
are meant to capture the relative ordering of subjects, verbs, and complements
in clauses which don't also involve word-order changing phenomena
such as topicalization, extraposition, subject-verb (or subject-auxiliary)
inversion, etc.  Eventual modules for such phenomena would need to
interact appropriately with the basic word-order modules.

The Grammar Matrix core grammar provides definitions of basic
head-complement and head-subject schemata which are consistent with
the Matrix implementation of compositional semantics
\cite{Fli:Ben:03}, as well as definitions of head-initial and
head-final phrase types.  These rules are also consistent with the
HPSG-style handling of long-distance dependencies implemented in the
Matrix.\footnote{We note that the long-distance dependency machinery
might be best treated as a module itself rather than part of the core
grammar.}  With this infrastructure, the implementation of the basic
word order modules was mostly a matter of creating subtypes joining
head-complement and head-subject schemata with the right types
specifying head/dependent order, in addition to creating instances of
those types as required by the \lkb\ parser.

The word order modules implemented so far handle the following
possibilities SOV, SVO, VSO, VOS, OVS, OSV, V-final, V-initial, and
free word order.\footnote{VOS, OVS and OSV are extremely rare among
the world's languages, with OSV being arguably unattested.
Nonetheless, we decided to implement support for all of them.}  In the
strict word orders, only one head-complement and one head-subject rule
are posited, with a strict ordering between them.  For V-final and
V-initial order (where the order of subjects and complements are
unconstrained), again we have only two rules, but they are allowed to
apply in either order.  For the free word order case, we posited both
head-initial and head-final head-complement and head-subject rules.
This alone is enough to capture the 6 possible word orders, but
without further work it leads to considerable spurious ambiguity in
the cases where there are arguments on both sides of the verb.  To
solve this problem, we adapted a strategy from the English Resource
Grammar for requiring attachment to the right before attachment to the
left.\footnote{We believe that this approach is linguistically
appropriate as long as the languages in question cannot be said to
have a VP constituent in OVS clauses.  Matrix grammars produce
maximally binary-branching structures.  This is convenient in that it
reduces the number of rules a grammar needs to encode, though at a
cost of arguably more complex trees.  We maintain that not all of the
constituents represented in such a tree need to be interpreted as
linguistically meaningful.  Conversely, however, any such grammar
should posit a constituent where the linguistic evidence says there
must be one.}
Figure~\ref{wordorderfig} shows the types files for the SOV and
V-final orders and the instance file which they both share.

\begin{figure*}[ht]
\begin{center}
{\tt\small
\begin{tabular}{l}
\hline
{\bf SOV.tdl}\\
comp-head-phrase := basic-head-1st-comp-phrase \& head-final.\\
subj-head-phrase := basic-head-subj-phrase \& head-final \&\\
   \phantom{foo}[ HEAD-DTR.SYNSEM.LOCAL.CAT.VAL.COMPS $\langle$ $\rangle$ ].\\
{\bf V-final.tdl}\\
comp-head-phrase := basic-head-1st-comp-phrase \& head-final.\\
subj-head-phrase := basic-head-subj-phrase \& head-final.\\
{\bf V-final-rules.tdl}\\
comp-head := comp-head-phrase.\\
subj-head := subj-head-phrase.\\
\hline
\end{tabular}
}
\end{center}
\caption{SOV and V-final basic word order modules}
\label{wordorderfig}
\end{figure*}


The core Matrix also provides a lexical type hierarchy
providing basic types for (among other things) nouns and verbs which
can be cross-classified with a set of types for various argument
structure patterns.  For the purposes of testing these modules,
we made a set of lexical types inheriting from these dimensions and
instantiated by a test lexicon.  In addition, to finesse the issue
of NP structure while still producing parses with well-formed semantic
representations (for testing via generation), we posited a place-holder
module providing an underconstrained determinerless NP rule.

We've been speaking of these modules as handling the order of
subjects, complements and verbs, but two caveats are in order.  First,
the head-complement and head-subject rules posited are not constrained
to any particular part of speech type. Thus, the system that is
produced models languages with consistent ordering of heads and
dependents, for heads of all parts of speech.  The Matrix provides a
type hierarchy of HEAD values that allows for statements over any
disjunction of head types, so it should be straightforward to produce
a system in which linearization is relativized to the head type, but
this remains future work.  Second, we began by considering clauses
with at most one complement, but of course that is an
oversimplification.  The rules as stated will handle an unbounded
number of complements, and realize them in an order determined by
their order on the head daughter's complements ({\sc comps}) list.

The next obvious step is to allow some flexibility in the order of
complements.  We believe that this will be best handled as a separate,
interacting modular system, as it appears that languages vary in the
extent to which they allow complements to reorder.\footnote{Contrast,
for example, English, which allows PP complements to reorder with each
other and with adjuncts fairly easily but keeps NP complements close
to their selecting heads, with Japanese, which generally allows free
ordering of complements and indeed free ordering between complements
and subjects (`scrambling').}  As a first step in this direction,
however, we have implemented a head-2nd-comp-rule which realizes the
second element of a head's {\sc comps} list, while passing the first
complement up to be realized at a higher level in the tree.  This rule
is instantiated for the free-word-order module only.  For sentences
with ditransitive verbs (and thus four elements: subject, verb, direct
object, and indirect object), a grammar with the free-word order
module will license all 24 possible orders of major clause
constituents, while assigning exactly one parse to each.

Finally, we note two further extensions in this domain which will need
to be handled to produce a system useful across a broad range of
languages.  The first is mixed word order, where one or more word
orders is arguably basic, but the choice between them is syntactically
constrained: either by head type (as discussed above) or by clause
type (matrix v.\ subordinate).  The second is a representation of the
pragmatic effects of word order, especially in free word-order
languages.\footnote{In languages with relatively fixed word order, the
displacement constructions which produce alternative orders provide a
place to encoded such pragmatic constraints.}


\subsection{Yes-no questions}

\subsection{Sentential negation}

The sentential negation modules handle three basic strategies for
sentential negation, as well as some hybrids: sentential negation
through verbal inflection; a V, VP or S adverb (pre- or post-head); a
selected adverb which appears as the complement of a verb (main or
auxiliary); inflection in combination with a selected adverb, either
optionally or obligatorily; and inflection in complementary
distribution with a negative adverb. We will illustrate two of these here.  

The negative inflection approach to sentential negation is implemented
by means of a lexical rule, shown in Figure~\ref{negrulefig}.  This
rule takes a verbal lexeme as its daughter and produces another verbal
lexeme with negative inflection as well as negative semantics.  The
constructional content ({\sc c-cont}) of this rule adds the negative
relation and the appropriate scopal constraint between that relation
and the verb's semantics. In the grammar generated using this module,
this type is instantiated by a lexical rule instance, specialized
according to user input as to whether the relevant inflection is a
prefix or a suffix, and as to the spelling of the suffix.  We assume
this spelling to be the underlying form which can then be passed to a
morphophonological component capable of handling non-concatenative
effects or other intricacies.  As further modules are developed, we
will need to tune the interaction of this rule with other lexical
rules to allow linguists to capture the ordering of the
processes.\footnote{Though note that the same generalizations could be
expressed in a morphological analysis component.  In this case
overgeneration on the part of the (morpho-)syntax would be merely
inefficient.}

\begin{figure}[ht]
{\scriptsize
\begin{verbatim}
negation-lex-rule := infl-ltol-rule &
[ SYNSEM [ LOCAL.CAT #cat,
           NON-LOCAL #non-loc ],
  DTR.SYNSEM [ LOCAL [ CAT #cat & [ HEAD verb ],
                       CONT.HOOK [ INDEX #ind,
                                   LTOP #larg,
                       XARG #xarg ]],
               NON-LOCAL #non-loc ],
  C-CONT [ HOOK [ INDEX #ind,
                  LTOP #ltop,
                  XARG #xarg ],
           RELS <! arg1-relation &
                   [ PRED '_neg_r_rel,
                     LBL #ltop,
                     ARG1 #harg ] !>,
           HCONS <! qeq &
                   [ HARG #harg,
                     LARG #LARG ] !> ]].
\end{verbatim}
}
\caption{Inflectional rule for sentential negation}
\label{negrulefig}
\end{figure}

In the negative adverb case, we posit a lexical type which inherits
from the Matrix type for scopal adverbs.  This type is specialized by user
input to be pre- or post-head and to attach to S, VP, or V.  Further
elaborations would allow for some underspecification along both dimensions.
Finally, a lexical entry is autogenerated using the negative adverb type
so created and a spelling provided by the user.


\section{Management of modules}
\label{management}

There are at least three partially conflicting pressures on the design
of a means of module management in this system: 1. From a developer's
perspective, it is desirable to capture as many generalizations as
possible across modules within a system so that common information is
factored out and coded only once, even if it is used in many
alternative modules. 2. From a user's perspective, it should be clear
what modules are available, what choices have been indicated so far,
and what implications those choices have for the generated grammar.
3. Again from a user's perspective, it should be relatively simple to
integrate updates to the Matrix (core grammar and modules) into a
grammar under development, without loss of information.\footnote{In
the worst case, the grammar developer might have to manually retweak
some module-generated part of the grammar for which the module had
been updated.  However, it should always be apparent to the user when
this is required.}  Similarly, it should be possible for the grammar
developer to revise a decision that was made early on (such as in the
type of negation strategy the language uses) and regenerate the
automatically generated portions of the grammar, without loss of
information.  These considerations have implications both for the
encoding of the underlying knowledge base that makes up the modules
and for the user interface designed to allow a linguist to interact
with them.

The simplest approach to these issues would be code each module as a
separate file or files, and devise a grammar-loading script with
suitable in-line comments indicating which files to include for which
linguistic effects.  This naive approach does not do well on the first
criterion: creating all of those separate files represents a great
deal of redundancy (sentential negation with preverbal VP adverbs is
very much like sentential negation with postverbal S adverbs, in terms
of the code that needs to be produced).  We could address this by
automatically generating the various files while still presenting them
to the user as static entities.  However, this won't scale up to
situations where the exact contents of one module need to be
customized in light of other modules chosen.  The naive approach fails
rather miserably on the second, UI-focused, criterion.  It would be
desirable to present the user with linguistic questions in a dynamic
fashion (presenting the choice of type of negative adverb only after
that general strategy has been chosen, for example).  This approach
does answer consideration (3) fairly well: it would simple enough to
provide a diff tool which compared user's versions of the files
included to off-the-shelf copies of the same modules, and alerted the
user to any local changes to files which had either been updated by
the Matrix developers or were involved in a grammar revision (change
of module selection).  However, there is still room for improvement:
some of the customization (e.g., spellings of various morphemes) could
more easily be automatically updated if that information were stored
where the system had direct access to it, rather than implicitly in
modified files.

A better solution would draw on linguist knowledge stored in a
separate set of files to automatically produce both the files that a
particular grammar needs to include and the script which loads them.
The UI for this system would be some kind of form or ``wizard'' which
allowed user input of both broad choices (e.g., which word order
system to use) and information to customize modules (is the negative
inflection a prefix or a suffix? how is it spelled?).  Such subsidiary
questions would only appear once the choice which makes them relevant
was made.  Furthermore, the system would record the user's answers, and
perhaps even be able to take a grammar and work out which answers must
have been given, as well as where files have been modified.  

In this paper, we leave the development of such a UI for future work,
and develop instead a command-line script (in Perl) which steps the
user through the linguistic questions (raising only those that become
relevant) and generates both the grammar files and the grammar-loading
script which the LKB uses to call them.  While the command-line UI is
certainly a far cry from ideal, it does succeed in making the task of
customizing the Matrix by making selections along the three dimensions
represented very quick work.  At the moment, the modules produce
several separate files, though it is perhaps an open question whether
it would be easier for users to further modify such automatically
generated grammars if the grammar code were more consolidated.

\section{Evaluation}

To evaluate the mutual consistency of the modules developed so far as
well as to illustrate their applicability to a interesting range of
languages, we developed abstract testsuites for N languages.  This
sample of languages is not intended to be representative, either
typologically or genetically, but is rather opportunistic.  The
testsuites are abstract in that we represent sentences in terms of
strings of part of speech tags rather than particular words, in order
to sidestep the problem of lexica for each language.

-- Table of languages and properties

-- Cite sources

-- Describe coverage, things about languages that are not covered

-- Describe the amount of work it takes to make a prototype with the
perl script, and compare to what it would take to do it from scratch

\section{Conclusion and outlook}

-- linguistic/scientific interest of modules: comparative analyses
across languages

-- ref to Montage? (or maybe save that for final version, when 
we're not trying to be anonymous)

-- mention uses of prototyping, for documentary linguistics, pedagogical
purposes, industrial-strength grammar engineering (including assisting
minority languages in producing resources relatively cheaply), and perhaps
benefits to machine-learning approaches -- can we imagine any?  something
along the lines of presenting typological properties in a machine-readable
way to improve ML algorithms which might need to be tuned differently for
different kinds of languages

-- Modularity in grammar engineering: observations about how what looks
like a module from a typological perspective is integrated with the rest
of the grammar from the perspective of any given language. Reiterate about
needing to be careful about cross-module consistency. Also :+. Interaction
between modules example: free-word-order rule had to say something about
head-modifier constructions to get the right attachment first thing to work.

-- Modularity in grammar engineering: reuse of bits across modules within
a system.  word order rule reuse, perl-script based specialization of the
same material with user's answers.


\section*{Acknowledgements}

%\section*{References}

\bibliographystyle{acl}
%\bibliographystyle{robbib}
\bibliography{modules}


\end{document}
