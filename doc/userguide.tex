\documentclass[12pt]{article}

\usepackage{avm}
\usepackage{lingmacros}
\usepackage{robbib}
\usepackage{rtrees}
\usepackage{fullpage}
\usepackage{pstricks}
\usepackage{tree-dvips}

\newcommand{\itsdb}{{\sf \lbrack incr tsdb()\rbrack}}
\newcommand{\hpsg}{\textsc{hpsg}}
\newcommand{\tsnlp}{\textsc{tsnlp}}
\newcommand{\pet}{\textsc{pet}}
\newcommand{\cheap}{\textsf{cheap}}
\newcommand{\tdl}{\mbox{${\cal T\!D\!L}$}}
\newcommand{\lingo}{{LinGO}}
\newcommand{\erg}{\textsc{erg}}
\newcommand{\viz}{viz.}
\newcommand{\ie}{i.e.}
\newcommand{\vs}{vs.}
\newcommand{\eg}{e.g.}
\newcommand{\qeq}{\textsc{qeq}}
\newcommand{\lkb}{\textsc{lkb}}
\newcommand{\mrs}{MRS}
\newcommand{\ale}{\textsc{ale}}
\newcommand{\tfs}{\textsc{tfs}}
\newcommand{\lfg}{\textsc{lfg}}
\newcommand{\xle}{\textsc{xle}}
\newcommand{\page}{\textsc{page}}
\newcommand{\ptb}{\textsc{ptb}}
\newcommand{\negra}{NeGra}
\newcommand{\tiger}{TiGer}
\newcommand{\testset}[1]{`\hspace*{-0.25ex}{\it #1}\/'}
\newcommand{\hpt}{$\cdot$}
\newcommand{\verbmobil}{VerbMobil}

\newcommand{\avmplus}[1]{{\setlength{\arraycolsep}{0.8mm}	
                       \renewcommand{\arraystretch}{1.1} %1.2
                       \left[ 			
                       \begin{array}{l}
                       \\[-2mm] #1 \\[-2mm] \\
                       \end{array} 		
                       \right]
                    }}
%
\newcommand{\att}[1]{{\mbox{\scriptsize {\bf #1}}}}
\newcommand{\attval}[2]{{\mbox{\scriptsize #1}\ {{#2}}}}
\newcommand{\attvallist}[2]{{\mbox{\scriptsize #1}\ {<{#2}>}}}
\newcommand{\attvaltlist}[2]{{\mbox{\scriptsize #1}\ {<{\myvaluebold{#2}}>}}}
\newcommand{\attvaltyp}[2]{{\mbox{\scriptsize #1}\ {\myvaluebold{#2}}}}
\newcommand{\attvalshrunktyp}[2]{{\mbox{\scriptsize #1}\ {\boxvaluebold{#2}}}}
\newcommand{\myvaluebold}[1]{{\mbox{\scriptsize {\bf #1}}}}
\newcommand{\boxvaluebold}[1]{{\fbox{\scriptsize {\bf #1}}}}
\newcommand{\ind}[1]{{\setlength{\fboxsep}{0.25mm} \: \fbox{{\small #1}} \:}}


\avmfont{\small\sc}
\avmvalfont{\footnotesize\it}

\newcommand{\es}{\enumsentence}
\newcommand{\ees}{\eenumsentence}
\newcommand{\fn}{\footnote}
\newcommand{\mc}{\multicolumn}
\newcommand{\tn}{\textnormal}
\newcommand{\ncon}{\nodeconnect}

\title{MRS in the LinGO Grammar Matrix:\\ A Practical User's Guide}
\author{Dan Flickinger, Emily M. Bender, and Stephan Oepen}

\begin{document}
\maketitle

\setcounter{tocdepth}{2}  % 2 = include subsections in table of contents
\tableofcontents
\newpage
\section{Introduction}

This paper is intended to serve as documentation for the semantic
aspects of the LinGO Grammar Matrix. We assume familiarity with the
specification of Minimal Recursion Semantics (MRS) given in
\citeboth{Cop:Fli:Sag:Pol:03} (see also \citeboth{Cop:Las:Fli:01}),
and focus instead on the practical aspects of writing grammars
that produce well-formed {\it mrs}s and of developing semantic
representations of particular linguistic phenomena within MRS.

Section \ref{backmot} gives some background on the LinGO Grammar
Matrix and MRS.  Sections \ref{basicobj}--\ref{synsem-i} describe
the implementation of MRS in the Matrix and how to extend it in
building a grammar of a specific language.  Along the way, we
provide touchstone linguistic examples and advice on best practice.
Section \ref{gen}
discusses considerations that arise in designing grammars for
generation as well as parsing.  Section \ref{verif} describes tools
and methods for verifying the well-formedness of {\it mrs}s.  Finally,
Section \ref{semisec} describes the Sem-I (`Semantic Interface'), a component
of the grammar that specifies how {\it mrs}s are to be interpreted.

\section{Background and Motivation}
\label{backmot}

The Matrix grammar starter-kit \cite{Ben:Fli:Oep:02} is a
language-independent core grammar designed to facilitate the rapid
initial development of grammars for natural languages, with
foundations solid enough to support steady expansion to broad coverage
of the linguistic phenomena in these languages.  Such grammars are
particularly valuable because they can assign semantic representations
to linguistic input, providing the foundation for applications which
require natural language understanding.  As such, a central component
of the Matrix is the collection of resources it contains for
simplifying the implementation of semantic composition within each
language and supporting the development of a standardized description
language for meaning representations, which can provide an effective
interface for practical applications.  The resources in the Matrix
further enable the meaning representations to keep pace as the
syntactic analyses of a grammar grow in complexity.

The goal of the Matrix grammar starter-kit is to provide the necessary
definitions of core linguistic types for words and phrases at a level
of generality which enables quick specialization to encode the
additional basic grammatical constraints for a particular language.
With this language-specific tuning, it should be possible to construct
a grammar within an afternoon which can be used to parse non-trivial
sentences of a given language, then use that same implementation as
the basis for the development over time of a semantically precise,
broad-coverage grammar.  The existing Matrix release also includes
software links and parameter settings for one particular grammar
development system, the LKB \cite{Copestake:02}, which includes an
efficient parser and generator, but grammars built on the Matrix can
be read and used by a number of other parsers
(cf.~\citeboth{Oep:Fli:Tsu:02}).

The Matrix is constructed within the formal system of typed feature
structures defined in \cite{Carpenter:92}, using the single operation
of unification to build phrases from the words and phrases they
contain.  Minimal Recursion Semantics was designed to enable semantic
composition using only this same unification of typed feature
structures, producing for each phrase or sentence a description of the
meaning representation sufficient to support logical inference.  The
type definitions for signs in the Matrix include a semantic component
which is an implementation of MRS, and more specifically of the
elaboration of a semantic algebra for MRS presented in
\citeboth{Cop:Las:Fli:01}.  In addition, MRS was designed to answer the
competing demands of expressive adequacy and computational
tractability, as well as to allow underspecification where it
facilitates computational applications, such as machine translation.
Thus Matrix-derived grammars are not only interesting as a means of
testing linguistic hypotheses, but also have the potential to be
integrated in applications which require natural language
understanding, including machine translation, automated email response
and speech prostheses.

Minimal Recursion Semantics is not a theory of semantics but rather a
system of semantic representations.  As such, in order to develop a
grammar of a particular language, there are any number of design
decisions that must be made about the content of the semantic
representations.  In this paper, we present some design decisions that
have emerged in work on two broad-coverage grammars the English
Resource Grammar (ERG: \citeboth{Flickinger00}) and the JaCY Japanese
grammar (JaCY: \citeboth{Sie:Ben:02}).
While the influences on these design decisions
are myriad, perhaps the most important guiding principle is the
following: The semantic representations produced should include all
grammatically relevant distinctions, while remaining as concise as
possible.  Thus, for example, in the Matrix as in the ERG, past tense
is represented simply as the attribute-value pair \mbox{[TENSE past]}
(on an event variable) rather than a more elaborate Reichenbachian
representation relating the event time to the time of utterance,
because this more elaborate representation can be unambiguously
derived from the more parsmonious one given.  (For more discussion,
see \S\ref{AgrTAM} below and {\S}6 of \citeboth{Cop:Fli:Sag:Pol:03}.)
We hope that the descriptions in the following sections of the design
decisions taken so far will provide guidance for grammar engineers
confronting new linguistic phenomena.

\section{Basic Semantic Objects}
\label{basicobj}

This section provides an overview of the semantic objects defined in the
Matrix, used in the specification of the values of {\sc cont} in signs, both
words and phrases.  Figure~\ref{mrsobj} gives a portion of the type hierarchies
under {\it avm} and {\it sort}, including all of the basic semantic objects.

\begin{figure}[ht]
\begin{center}
{\it\small
\begin{tree}
\psset{treefit=loose}
\br{*top*}{
\br{avm}{	
 {\br{mrs}{\lf{psoa}\lf{\ \ \ \ \ \ \ \ \ nom-obj}}}
 \lf{lexkeys}
 \lf{keys}
 {\br{relation}{\tlf{\ldots}}}
 \lf{qeq}
 \lf{   hook}
 \br{semarg}{\br{individual}{\tlf{\ldots}}\lf{\ \ \ \ handle}}
 \lf{tam}
 \lf{   png}}
\br{sort}{\lf{semsort}}}
\end{tree}
}
\end{center}
\caption{Type Hierarchy of Basic Semantic Objects}
\label{mrsobj}
\end{figure}

\subsection{\it mrs}
\label{mrssec}

The flat semantic representations assigned to each word
or phrase in MRS comprise three components: 

\begin{enumerate}
\item {\sc rels} - a bag of atomic predications, each
with a `handle' (used to express scope relations) and
one or more roles;
\item {\sc hcons} - a set of handle constraints which
reflect syntactic limitations on possible scope relations
among the atomic predications;
\item {\sc hook} - a group of distinguished externally visible
attributes of the atomic predications in {\sc rels}, used
in combining the semantics of this sign with the semantics
of other signs.
\end{enumerate}

Thus objects of type {\it mrs} (i.e., the value of the features
{\sc cont(ent)} and {\sc c(onstructional)-cont} of {\it sign}s in
the Matrix) are constrained as in (\ref{mrs-cons}).

\es{\label{mrs-cons}
\begin{avm}
{\it mrs}: \[ hook & hook\\
              rels & diff-list\\
              hcons & diff-list \]
\end{avm}
}
%
The value of {\sc rels} is a difference list\footnote{These lists are
implemented as difference list to allow for appending without
relational constraints.} of {\it relation}s (\S\ref{reltypesec}) and
the value of {\sc hcons} is a difference list of {\it qeq}s
(\S\ref{qeqsec}).  The value of {\sc hook} is a feature structure of
type {\it hook} (\S\ref{hooksec}).

The type {\it mrs} also has two subtypes {\it nom-obj} and {\it psoa},
following \citeboth{Pol:Sag:94}, corresponding to semantic representations of 
nominal signs (with a distinguished {\it ref-ind}) and predicative signs
(with a distinguished {\it event}).  The types {\it ref-ind}
and {\it event} are described in \S\ref{semargsec} below.  This distinction
is encoded in the following two constraints:\footnote{On {\it hook}, 
see \S\ref{hooksec} below.}

\es{\label{psoa}
\begin{avm}
{\it psoa}: \[ hook & \[ index & event \]\]
\end{avm}}

\es{\label{nomobj}
\begin{avm}
{\it nom-obj}: \[ hook & \[ index & index \]\]
\end{avm}}

\subsection{{\it relation}}
\label{reltypesec}

The heart of an {\it mrs} is a bag of elementary predications,
implemented here as a difference list of objects of type {\it
relation} -- i.e., the value of the feature {\sc rels}.\footnote{Note
\label{dlfn} that the Matrix doesn't include typed difference lists,
so that the value of {\sc rels} is in fact only constrained to be a
difference list without further restriction -- it could just as well be a
difference list of {\it synsem}s.  By convention, and in order to
produce well-formed semantic representations, however, it is always a
difference list of {\it relation}s.} All relations bear values for the
three features introduced on the type {\it relation}, as shown in
(\ref{relation}):

\es{\label{relation}
\begin{avm}
{\it relation}: \[ lbl & handle\\
                   pred & string\\
                   wlink & list \]
\end{avm}
}

\subsubsection{\sc lbl}

The value of {\sc lbl} is a {\it handle}, which is used to express
scope relations.  The {\sc lbl} value of one relation may be identified
with (i) the {\sc lbl} value of one or more other relations, (ii) a
role within some other relation, (iii) a value within a handle
constraint ({\it qeq} -- see \S\ref{qeqsec} below), and/or the value
of {\sc ltop} in a {\it hook} (see \S\ref{hooksec} below).  Relations sharing a
{\sc lbl} value are interpreted as conjoined.  For example, in {\it
The big dog slept.}, the relations introduced by {\it big} and {\it
dog} will share a {\sc lbl} value.  {\it Qeq} constraints and direct
identification of {\sc lbl} values with argument roles in other
relations are used to express scopal interactions among relations.  For
the most part, this is achieved with {\it qeq} constraints, which identify
semantic argument positions where quantifiers can intervene scopally when
underspecified {\it mrs}s are resolved to fully scoped representations.
For those handle argument positions where quantifiers cannot intercede,
direct reentrancy between a {\sc lbl} value and a role is employed.  For an
example, see \S\ref{msgsec} below.  Note that we will often refer to
the {\sc lbl} value of a relation as its handle.

\subsubsection{\sc pred}

The value of {\sc pred} is a string, which serves to distinguish
particular relations.  Earlier versions of the ERG
included a separate type for each distinct relation, leading to a very
flat (and very large) type hierarchy.  We have since found it preferable 
to distinguish
relations via the string-valued {\sc pred} feature, and to reserve the
subtypes of {\it relation} (see \S\ref{relarg}) for those types
that introduce features.  This change also opens the door to more
interesting re-entrancies of {\sc pred} values, explored in the ERG for
the semantics of degree specifiers, and perhaps also useful for some
treatments of coordination or gapping.

For compatibility with RMRS (FIX: ref) and software designed to
integrate deep and shallow processing, {\sc pred} values should
conform to the following templates:

\es{
\begin{tabular}[t]{ll}
{\tt \_orth\_pos\_sense\_rel} & (lexically introduced predicates)\\
{\tt orth\_pos\_sense\_rel} & (abstract predicates introduced by constructions)
\end{tabular}
}

The {\tt orth} component is a string corresponding to the (stem)
orthography of the lexical entry, at least for all open-class words,
and typically also for closed-class words.  By using the stem orthography,
we enusre that predicate names used in {\it mrs}s produced by deep
grammars will be interoperable with predicate names used in {\it mrs}s 
produced by robust shallow processors, which name the predicates based
on (lemmatized) forms in the input.

The {\tt pos} component is one
of a closed set of single lowercase letters interpreted as follows:\\

\es{
\begin{tabular}[t]{ll}
n & noun \\
v & verb \\
j & adjective \\
r & adverb \\
p & preposition \\
q & determiner (quantifier) \\
m & message \\
x & all other closed-class predicates
\end{tabular}
}
%
We use this POS-based information (such as might be accessible from a
POS-tagger) for coarse-grained sense distinctions.  Finer grained
distinctions can be made (in a precision grammar) via the {\tt sense}
component.  The {\tt sense} component can consist of any sequence of
characters (letters, numbers, etc.), excluding the underscore which is
used to separate the components of the name.  In the ERG, verb
particle constructions are handled semantically by having the verb
contribute a relation particular to that combination.  We distinguish
these relations by placing the particle's orthography in the {\tt
sense} field. Unlike the other components, the {\tt sense} component
is optional, and if omitted, its separating underscore is also
omitted.

Every relation and predicate name ends in {\bf \_rel}, for the
convenience of the grammar writer, particularly to avoid possible
namespace collisions.  This suffix (and the leading underscore) can of
course be suppressed by MRS display methods if desired.

So for example, the following predicate names are correct for the 
corresponding words:\\

\es{
\begin{tabular}[t]{ll}
{\it aardvark} & {\bf \_aardvark\_n\_rel}\\
{\it bank}     & {\bf \_bank\_n\_2\_rel}\\
{\it bank}     & {\bf \_bank\_v\_turn\_rel}\\
{\it look}   & {\bf \_look\_v\_up\_rel}\\
\end{tabular}\\
}

Finally, one further detail of formatting should be mentioned:
Words with single lexical entries whose orthography is conventionally
spelled with a space, such as the English use of {\it ad hoc}, appear with the
whole orthography in the {\tt orth} component, but with the space(s)
replaced by the plus sign.  So the following example is also correct:\\

\es{
\begin{tabular}[t]{ll}
{\it ad hoc} & {\bf \_ad+hoc\_j\_rel}\\
\end{tabular}
}

\subsubsection{\sc wlink}

The feature {\sc wlink} serves to link the relation to an element in
the input string. 
Since this feature does not interact with the rest of the semantic
composition machinery in the Matrix, we will omit it from our AVM
descriptions in the rest of this paper.
%[FIX (oe): Something about how this works, which
%systems support it, what a grammar engineer needs to know about it?]

\subsection{{\it qeq}}
\label{qeqsec}

As mentioned above, scopal relations in MRS are represented via
handles, which appear as the value of the feature {\sc lbl} and also
as the value of certain roles within scopal relations (e.g., the {\sc
body} and {\sc rstr} values of quantifiers -- see \S\ref{quantsec}).
However, for many applications (including machine translation), fully
specified scope relations are not required.  Furthermore, any surface
string with multiple noun phrases (and therefore multiple quantifiers,
overt or implicit) is going to be ambiguous with respect to scopal
relations.  Rather than build separate parses for each scoping, and
potentially have to choose between them in a given application, it is
preferable to leave scopal relations underspecified, to the extent
that the grammar doesn't in fact constrain them.\fn{For discussion of
underspecification of scope, see \citeboth{Cop:Fli:Sag:Pol:03} and
references given there.}  In Matrix-derived grammars, following MRS 
specifications, this is achieved as follows:

\begin{enumerate}
\item The {\sc body} (i.e., scope) of all quantifiers is left unconstrained.
\item Most other handle-taking argument positions are not directly linked 
to the handle of some relation.
\item Rather, the two are related via a {\it qeq} (`equality modulo 
quantifiers') constraint.
\end{enumerate}

\noindent
Thus the {\sc body} value of quantifier relations can be resolved in
such a way that the quantifiers `float in' wherever there's a `space'
left by a {\it qeq} constraint.

In the implementation, this is represented via the feature {\sc
hcons}.  The value of {\sc hcons} is a difference list (again,
representing a bag) of {\it qeq}s.\fn{Once again, the difference list
is not typed.  See note~\ref{dlfn}.}  {\it Qeq}s, in turn, are
constrained as follows:

\es{
\begin{avm}
{\it qeq}: \[ harg & handle\\
	      larg & handle \]
\end{avm}
}

\noindent
The {\sc harg} is identified with the handle-taking argument position
and the {\sc larg} is identified with the {\sc lbl} (handle) of the outscoped
relation.  Examples of parts of the grammar that impose such constraints
are given in \S\ref{hconssec} below.

While the MRS specification in \citeboth{Cop:Fli:Sag:Pol:03} leaves
open the possibility of different kinds of handle constraints, only
{\it qeq} constraints have proven necessary so far for wide-coverage
grammars of English (ERG) and Japanese (JaCY), and so only
{\it qeq} constraints are currently implemented in the 
Matrix.\footnote{However, a large grammar implementation for German (FIX: ref 
Verbmobil book?) makes crucial use of {\it leq} (less than or equal) scopal
constraints, and recent work on extending MRS representations to robust 
processing (FIX: ref Copestake et al)
also employs {\it leq} constraints.}

\subsection{{\it hook}}
\label{hooksec}

Where the values of {\sc rels} and {\sc qeq} give a rich representation of 
the semantics of any given sign (word or phrase),
more information is needed in order to be able to combine the semantic
representations of signs in order to compositionally build
semantic representations of larger phrases.  For example, consider the
partial {\it mrs}s produced by the ERG for the signs {\it the dog} and 
{\it barks} in
(\ref{db1}):\fn{Following \lkb\ conventions, difference lists are
represented with the brackets $\langle$! !$\rangle$, consistent with the
abbreviatory conventions employed in the TDL (FIX ref)
syntax adopted in the \lkb.}

\ees{\label{db1}
\item \begin{avm}
\[ {\sc rels} & \q<\tn{!} \[ \avmspan{\it quant\_rel}\\ 
		  lbl & \@1\\
                  pred & {\bf \_def\_q\_rel}\\
                  arg0 & \@2\\
                  rstr & \@3 
                  body & \@4 \] ,
               \[ \avmspan{\it noun\_rel}\\
		  lbl & \@5\\
                  pred & {\bf \_dog\_n\_rel}\\
                  arg0 & \@2 \] \tn{!}\q>\\
   hcons & \q<\tn{!} \[ {\it qeq}\\
	           harg & \@3\\
                   larg & \@5 \] \tn{!}\q> \]
\end{avm}
\item \begin{avm}
\[ {\sc rels} & \q<\tn{!} \[ \avmspan{\it arg1\_ev\_rel}\\
                  pred & {\bf \_bark\_v\_rel}\\
                  arg0 & event\\
                  arg1 & semarg \] \tn{!}\q>\\
   hcons & \q<\tn{!} \tn{!}\q> \]
\end{avm}
}
   
In composing the {\it mrs} for the larger phrase {\it the dog barks},
we would like to identify the {\sc arg0} of the {\bf \_dog\_n\_rel} with the
{\sc arg1} of the {\bf \_bark\_v\_rel}.  But how can the relevant parts of
the grammar (in this case, the lexical entry for {\it bark}) gain
access to the right value of the right relation?  While the value of
{\sc rels} is implemented as a difference list, it is notionally to be
treated as a bag, so it would be unprincipled to leverage position on
the list.  In fact, it would also be impractical: the list position of the
relation contributed by the head noun in a noun phrase
is affected by what else there is in the NP.  For example, in {\it the
big dog}, the {\bf \_big\_j\_rel} would intervene (in this implementation)
between the {\bf \_def\_q\_rel} and the {\bf \_dog\_n\_rel}.  The solution is 
to use the value of {\sc hook} to `publish' or make visible externally
just those elements of the {\it mrs} that other constituents need
access to.

Thus the value of {\sc hook} represents a hypothesis about which 
information may be accessed externally.  The Matrix provides for
three compositionally relevant properties of a sign's semantics, encoded as
features for objects of type {\it hook}:

\es{
\begin{avm}
{\it hook}: \[ ltop & handle\\
               index & individual\\
               xarg & individual \]
\end{avm}
}

\noindent
The value of {\sc ltop} is the local top handle, the handle
of the relation(s) with the widest scope within the constituent, modulo
quantifiers.   This attribute
is accessed by semantic heads in phrasal constructions in order
to impose further scopal constraints involving that handle when composing
the semantics of the phrase.

The value of {\sc index} is the distinguished non-handle variable supplied
by the sign, identified with the {\sc index} of the semantic head daughter,
and usually the {\sc arg0} of the main relation introduced by the syntactic 
head of
the constituent.  If the {\it mrs} is a {\it nom-obj}, this will be a
{\it ref-ind} (referential index).  If the {\it mrs} is a {\it psoa},
this will be an {\it event}.  (See \S\ref{mrssec} above on {\it psoa}
and {\it nomobj} and \S\ref{semargsec} below on {\it ref-ind} and {\it
event}.)  If the sign is a modifier like a prepositional
phrase or an adjective phrase, its {\sc index} will be determined by the
constraints on the phrase to be modified.
This information is accessed by semantic heads
in order to identify indices (including event
indices) with (non-scopal) argument positions in predications: e.g.,
the {\sc arg1} (barker) of the {\bf \_bark\_v\_rel} or the {\sc arg1}
(modified event) of the relation for an intersective adverb like {\it happily}.

Finally, the value of {\sc xarg} (mnemonic for `external argument') is
the index of the single argument in a phrase (in accusative languages,
typically the subject) which can be controlled.  This information will
be accessed by semantic heads in raising and control constructions,
open adjuncts, and constructions like English tag questions.  See
\S\ref{xargsec} for further exemplification.

\subsection{{\it semarg}, {\it tam}, and {\it png}}
\label{semargsec}

The values of {\sc lbl}, {\sc harg}, {\sc larg}, and all role features
(e.g., {\sc arg0} etc., see \S\ref{relarg}) are objects of (subtypes
of) type {\it semarg}.  This type introduces the non-linguistic feature {\sc
instloc}, which is used in generation and whose value should never be
further constrained by the grammar.  The hierarchy below {\it
semarg} is shown in Figure~\ref{semarghier}.

\begin{figure}[ht]

\begin{center}
\begin{tabular}{ccccccc}
\mc{4}{c}{\node{1}{\it semarg}} & & \\[.5cm]
\node{2}{\it handle} & \mc{4}{c}{\node{3}{\it individual}} & & \\[.5cm]
\mc{2}{r}{\node{4}{\it index}} & \mc{5}{c}{\node{5}{\it event-or-ref-index}}\\[.5cm]
\node{6}{\it expl-ind} & \mc{2}{c}{\node{7}{\it ref-ind}} & \mc{2}{c}{\node{8}{\it conj-index}} & \mc{2}{c}{\node{9}{\it event}}\\[.5cm]
& & \mc{2}{c}{\node{10}{\it conj-ref-ind}} & \mc{2}{l}{\node{11}{\it conj-event}} &\\
\end{tabular}
\end{center}

\ncon{1}{2}
\ncon{1}{3}
\ncon{3}{4}
\ncon{3}{5}
\ncon{4}{6}
\ncon{4}{7}
\ncon{5}{7}
\ncon{5}{8}
\ncon{5}{9}
\ncon{7}{10}
\ncon{8}{10}
\ncon{8}{11}
\ncon{9}{11}

\caption{Type hierarchy rooted in {\it semarg}}
\label{semarghier}
\end{figure}

There are two subtypes of {\it semarg}.  The first, {\it handle},
is the type of the value of all 
handle-taking features ({\sc lbl}, {\sc harg}, {\sc larg}, {\sc marg},
{\sc body}, {\sc rstr}), and can be the value type for a semantic role
filling one of the features {\sc arg1, arg2, ...}.  
There are no subtypes of {\it handle}, nor is it
anticipated that grammar engineers will need to add any in the course
of grammar development.  Furthermore, there are no features introduced
on the type {\it handle} (although it inherits {\sc instloc} from
{\it semarg}).

The other immediate subtype of {\it semarg} is {\it individual},
which includes expletive indices ({\it expl-ind}), event indices (or
event variables) {\it event} and ordinary referential indices ({\it
ref-ind}).  In addition, the Matrix provides for coordinate
indices ({\it conj-ind} with subtypes {\it conj-event} and {\it
conj-ref-ind}) which are introduced by coordination constructions (see
\S\ref{coordsec}).  Note that {\it conj-event} and {\it conj-ref-ind}
inherit from {\it event} and {\it ref-ind}, respectively.  This
ensures that they are compatible with any environment that requires an
{\it event} or {\it ref-ind}, and, conversely, that it is not possible
to specifically select a non-coordinated event or referential
index.\footnote{The hierarchy could of course be extended to allow
such selection, but we expect that it will not be required.}

The contrast between {\it expl-ind} and {\it event-or-ref-index} is
used to constrain the distribution of expletive NPs (e.g., English
{\it it} and {\it there}, German {\it es}, or French {\it
il}).\footnote{Although in general we intend the types given in the
Matrix proper (as opposed to separate modules that will eventually
come with it) to be largely language-independent, 
{\it expl-ind} is probably not useful in languages
which allow pro-drop to the extent that Japanese does, and which therefore
have no use for expletive elements.}  Such expletive pronouns are given
an {\sc index} value of type {\it expl-ind} (which is exceptionally not
linked to any argument position in their main relation, since they
don't introduce any relation).  Selecting heads require dependents with
contenful {\sc index} values (i.e., {\it event-or-ref-index}s, or
some subtype thereof).  In languages like English (and perhaps Dutch) with
multiple expletive pronouns, subtypes of {\it expl-ind} can be added
to distinguish the different pronouns.  In the ERG, we have found
this technique useful not only for implementing the selection of 
particular expletive pronouns by particular heads (e.g., {\it rains} vs.\
existential {\it be}), but also for pronoun matching in tag questions
(see \citeboth{Ben:Fli:99}).

The type {\it individual} introduces the feature {\sc sort}, which can be
used to identify the grammatically relevant semantic sort (if any) of the
sign, for use in semantic selection of this sign as a dependent in a larger
phrase.  The value of this attribute is of type {\it semsort}, discussed
in \S\ref{keysec}.

The types {\it event} and {\it ref-ind} each introduce one feature,
as shown in (\ref{evref}):

\ees{\label{evref}
\item \begin{avm}
{\it event}: \[ tam & tam \]
\end{avm}

\item \begin{avm}
{\it ref-ind}: \[ png & png \]
\end{avm}
}
%
The features {\sc tam} and {\sc png} encode tense-aspect-mood
information (a property of events) and agreement information (person,
number, and gender; properties of referential indices, cf.\
\citeboth{Pol:Sag:94}), respectively.  In the current version of the
Matrix (v 0.5), the type {\it tam} (unsurprisingly) introduces the
features {\sc tense}, {\sc aspect}, and {\sc mood}.  This may be too
strong a constraint, since languages may well conflate two or more of
these properties; see the discussion of the {\it png} type immediately
following.  We will have more to say about the analyses of TAM and agreement in
\S\ref{AgrTAM} below.

\es{
\begin{avm}
{\it tam}: \[ tense & tense\\
              aspect & aspect\\
              mood & mood \]
\end{avm}
}
%
The type {\it png}, on the other hand, introduces no features, though 
it is intended to provide the locus for constraints on (semantic) person, 
number, and gender.  These three dimensions are not necessarily distinct
for a given language, as seen in English where person and number are
usually conflated morphologically, motivating a basic distinction between
third-singular and non-third-singular inflectional types, with further
subtypes for non-third-singular.\footnote{Cf. \citeboth{Flickinger00} for
discussion of this part of the type hierarchy.} Thus in English the most 
natural feature structure for the {\it png} type is to have a merged 
person-number {\sc pn} attribute along with a gender {\sc gen} attribute.
Clearly, in other languages separate attributes for person and number are
well motivated, leading us to leave the elaboration of the {\it png} type
as language-specific for now.

\subsection{{\it keys}}
\label{keysec}

The next type of object to consider is {\it keys}.  This type is
introduced to serve as the value of the {\sc head} feature {\sc keys}.
The purpose of {\sc keys} is to provide constraints for semantic
selection of dependents (complements, specifiers, subjects, and
modifiers).  The attributes in {\it keys} include two called {\sc key}
and {\sc altkey} for constraining the semantic sorts of the dependent
phrase, and one called {\sc message} for constraining types of clausal
dependents. Since the {\sc keys} attribute is a {\sc head} feature,
the grammar ensures that these semantic sort properties of a phrase
propagate up the syntactic head path.

The type {\it keys} is constrained as shown in (\ref{keys}):

\es{\label{keys}
\begin{avm}
\[ key & semsort \\
   altkey & semsort \\
   message & message \]
\end{avm}
}

The feature {\sc key} serves as a pointer to the semantic sort of a phrase
where a semantic sort is one of the subtypes defined for the %distinguished 
type {\it semsort}, described in \S\ref{semsort} below.
The value of
this feature is often but not always identified with the value of the 
semantic feature of a sign given by the path 
{\sc SYNSEM.LOCAL.CONT.HOOK.INDEX.SORT}.  Mismatches occur because 
the syntactic and semantic head paths can differ (cf.\ \S\ref{semheadsec}), or
because lexical entries idiosyncratically assign different values.
% since the semantic sort of a sign
%usually corresponds to the type reflected in head-dependent collocations,
%but there can in principle be exceptions.

Since generalizations about semantic selection may make reference to more than
one such semantic sort of a sign, the Matrix provides a second {\sc keys}
attribute called {\sc altkey}.  The idea here is that a modifier, for example,
may constrain the phrase it modifies along one dimension of semantic selection,
while a verb taking that same modified phrase as a complement may need to 
constrain it along a second dimension.  

The third feature of objects of type {\it keys} is {\sc message}.
This attribute has as its value a subtype of the type {\it message} 
or the type {\it no-msg}, depending on whether the phrase is clausal
or non-clausal.  {\it Message}s are our representation of the 
semantic types of clauses and are described in more detail in
\S\ref{msgrelsec} below.  For clausal phrases, the value of {\sc message}
will be a pointer to the relevant message relation in the {\sc rels} list
of that phrase.  

Note that as a head feature, the value of {\sc keys} on the mother node of a
headed-phrase will be identified with the {\sc keys} value of the head 
daughter.  For non-headed phrases such as coordinate constructions, each
such construction type will have to stipulate the values for the attributes
in {\sc keys} on the mother node.  These values may come from one or the other
of the daughters, or may be supplied directly by the construction itself.




\subsection{{\it semsort}}
\label{semsort}

The type {\it semsort} is the root of a hierarchy of sorts which
serves to represent grammaticized semantic distinctions which are used
in the selection of dependents (subjects, complements, specifiers, and
modifiers).  We expect it to include a small hierarchy of
grammatically salient semantic types such as {\it animate} or {\it
time}, as well as more specific types to support selection of
closed-class items, such as preposition selection by verbs or
auxiliary matching in English tag questions \cite{Ben:Fli:99}.  Note
that {\it semsort} is made a subtype of {\it sort} to make explicit
the claim that these types used for semantic selection do not
themselves introduce attributes of their own.
% Dan - and why do we predict that?

The ERG illustrates several uses of semantic selection, which motivate
some particular subtypes of {\it semsort}.  These sorts in the ERG
include at least the human/nonhuman distinction reflected in the
choice of relative pronoun ({\it the person *which/whom I met}), and
the various semantic subtypes introduced by prepositions (temporal,
locative, directional, stative, etc.).  Verb-preposition dependencies
in English, for example, can be encoded by having the verb constrain
the {\sc key} value of its PP complement.  In some cases, the
selecting verb might not be looking for a particular preposition's
{\sc key} value, but rather impose a more abstract constraint on a
complement's key, such as the PP complement for the English verb {\it
put}, whose {\sc key} value might be constrained to those introduced
by a subclass of locative prepositions, to preclude analyzing
e.g. {\it *Kim put the chair for Sandy.}  Thus both the leaves and
the intermediate types in the {\it semsort} hierarchy can be useful.

The subhierarchy under {\it semsort} will be language-specific,
reflecting grammaticized semantic properties motivated by
constructions in that language.  The analysis of Japanese numeral
classifiers, for example, can be implemented using these semantic
sorts, as can the quite idiosyncratic constraints for English on
preposition selection with temporal nouns.  Here, for example, noun
phrases like {\it Tuesday} or {\it the fifteenth} (denoting days of
the week or days of the month) introduce the semantic sort {\it day}
which is selected by (the relevant lexical entry for) the preposition
{\it on} but not {\it in}, to admit {\it Kim arrived on Tuesday} but
not {\it *Kim arrived in Tueaday}.  Such idiosyncratic collocational
constraints often reflect natural semantic distinctions, but are not
predictable cross-linguistically.

% Maybe move this somewhere else? --EB
Note that, as strings, {\sc pred} values are not organized into a hierarchy.
Furthermore expect to only need to identify particular relations
(via {\it semsort}s which correspond to specific {\sc pred} values)
in a reasonably small number of closed classes of lexical items.
We therefore see now value in providing an external pointer to the
{\sc pred} value of the main predication contributed by the lexical
head (syntactic or semantic) of a phrase.


\subsection{{\it lexkeys}}
\label{lexkeysec}

The final type of object to consider here is the type {\it lexkeys}, which
is defined in the Matrix to provide attributes which are not linguisticslly
significant, but which provide some convenient shorthand notation for the
grammar writer when defining the lexical type hierarchy.  In particular,
this type introduces two attributes that simplify the expression of
constraints on relations introduced by a lexical type, and two attributes
that point to the {\sc key} attribute of complements of the lexical type.
The grammar writer can decide whether or not to make use of these shorthand
attributes (whose names bear a leading double dash as a reminder that they
are only abbreviations for longer path names).  
The type {\it lexkeys} is the value of the {\sc local}  feature {\sc lkeys},
and nothing in the Matrix propagates the value of this feature up
to phrases.  Thus, the features on {\it lexkeys} are only
used (if at all) in simplifying the definitions of lexical entries.

The type {\it lexkeys} is constrained as follows:

\es{
\begin{avm}
{\it lexkeys}: \[ {-}{-}keyrel & relation\\
    {-}{-}altkeyrel & relation\\
    {-}{-}compkey & semsort\\
    {-}{-}ocompkey & semsort \]
\end{avm}
}

\noindent
The features {\sc {-}{-}compkey} and {\sc {-}{-}ocompkey} provide
pointers to the {\sc key} values of two complements of a lexical
entry.  The relationship
between {\sc {-}{-}compkey}/{\sc {-}{-}ocompkey} and {\sc keys.key} of
the relevant complements will be established in lexical types.\fn{In the
case of the entry for {\it abstain} given here, the relevant constraint
is on the type {\it unsat\_two\_arg\_subst}, which is a supertype of
the {\sc synsem} value of {\it v\_empty\_prep\_intrans\_le}.}
This allows specific lexical entries to do semantic selection of
complements by simply constraining their own {\sc {-}{-}compkey} and/or
{\sc {-}{-}ocompkey} values, as in the following lexical entry adapted
from
the ERG:

\es{
\begin{avm}
{\it abstain\_v1}: \[ \avmspan{\it v\_empty\_prep\_intrans\_le}\\
		     stem & \q< \textnormal{``abstain''} \q>\\
                     synsem & \[ local \[ cat.heat.keys.key & {\bf \_abstain\_v\_from\_rel}\\
		                          lkeys.{-}{-}compkey & \_from\_p\_rel\_s \]\]\]
\end{avm}
}

The other two features {\sc {-}{-}keyrel} and {\sc {-}{-}altkeyrel} provide
pointers to each of two relations introduced by a lexical entry, for easier
definitions of constraints on values of those relations within the lexical
type hierarchy.  Typically, the value of {\sc {-}{-}keyrel} will be a pointer
to the relation in the {\sc rels} list of a lexical entry which introduces
its {\sc index} value as the {\sc arg0} of that relation, so it will be
the {\it noun\_rel} for a noun, or the {\it event\_rel} for a verb.  The value of
{\sc {-}{-}altkeyrel} will be unbound for for most lexical entries, since
most introduce a single relation in the {\sc rels} list, but for
an entry which introduces more than one relation, this attribute provides
a convenient pointer to a second relation on the {\sc rels} list.


\subsection{Summary}

This concludes our tour of the basic semantic objects defined in the
Matrix.   In this section we have briefly touched on some aspects
of the linguistic analyses that motivate each type of object.  The
following sections flesh out these linguistic analyses in more detail.

\section{Relations and Argument Features}
\label{relarg}

This section describes the various subtypes of {\it relation} posited
in the Matrix and provides guidelines for introducing new subtypes of
{\it relation}.  The top of the type hierarchy below {\it relation}
is shown in Figure~\ref{relhier}.

\begin{figure}[ht]
\begin{center}
{\it
\begin{tree}
\br{relation}{\br{arg0\_rel}{\br{noun\_rel}{\lf{named\_rel}}
	   	             \br{event\_rel}{\tlf{\ldots}}
		             \lf{quant\_rel}
			     \lf{\ \ \ adv\_rel}}
			     \br{\ \ message}{\tlf{\ldots}}
		             \br{subord\_or\_conj\_rel}{\tlf{\ldots}}}
\end{tree}
}
\end{center}
\caption{Partial type hierarchy below {\it relation}}
\label{relhier}
\end{figure}

In the following subsections we will describe
ordinary predicates (i.e., the subtypes of {\it arg0\_rel} introduced by
nouns, verbs, prepositions, adjectives and
adverbs -- \S\ref{ordpred}), and the special relation types defined
for quantifiers (\S\ref{quantsec}), messages (\S\ref{msgrelsec}), and
subordination and coordination (\S\ref{coordsec}).

While we expect that grammar development for particular languages will
require the addition of some abstract relation types to the set
proposed here, we recommend not positing a type for each lexical
relation, but rather using the feature {\sc pred} (see
\S\ref{reltypesec} above) to distinguish different lexical relations
of the same type.  New relation types are merited only in two circumstances:
(i) when a feature needs to be introduced that is relevant for some relations
but not others; or (ii) when the values of one or more attributes of an 
existing relation are consistently constrained in the same way across multiple
contexts in the grammar.

\subsection{Ordinary Predicates}
\label{ordpred}

\subsubsection{{\it arg0\_rel}, {\it noun\_rel}, {\it event\_rel}, {\it adv\_rel}}
\label{arg0sec}

The vast majority of lexical entries introduce only a single relation,
and furthermore one that is an instance of {\it arg0\_rel} or one of its
subtypes, including {\it noun\_rel}, {\it event\_rel}, and {\it adv\_rel}.  
The type {\it arg0\_rel} is constrained as follows:

\es{
\begin{avm}
{\it arg0\_rel}: \[ arg0 & individual \]
\end{avm}
}

Thus all open class lexical items introduce a relation with the {\sc
arg0} role.  The value of this role is an {\it individual}.  For nouns
and verbs, the {\sc arg0} of the key relation (i.e., the {\sc {-}{-}keyrel} value) should be identified with
the {\sc hook.index}, and this identification should be done on a general
supertype.

For nouns, it will be a (referential) index which serves as a pointer
to the entity referred to by the NP.  This
same index is also the value of the {\sc arg1} feature in any relations
introduced by intersective modifiers of the head noun, and also the value of 
a role feature in the key 
relation of any predicate selecting the NP as a semantic argument.  The type
{\it noun\_rel} is constrained appropriately:

\es{
\begin{avm}
{\it noun\_rel}: \[ arg0 & ref-ind \]
\end{avm}
}
%
As discussed in \S\ref{semargsec} above, objects of type {\it ref-ind}
bear the feature {\sc png}.  This means that the agreement information
associated with a noun will also be found in {\sc lkeys.{-}{-}keyrel.arg0},
but this information doesn't propagate up to the phrase, and so agreement
information should always be accessed through the {\sc cont.hook.index}.

For verbs, which introduce relations of type {\it event\_rel}, the
value of {\sc arg0} is the event variable identified as the {\sc index} value.
Again, this
{\it event} will also show up as the value of a role feature in
modifier relations such as those expressed by adverbs and by PPs modifying
verbal phrases.  Other elements that might appear to notionally take
events as arguments (e.g., clause-embedding verbs) actually take a
handle (e.g., of the message that embeds the event relation) as their
argument.  The type {\it event\_rel} is therefore constrained as follows:

\es{
\begin{avm}
{\it event\_rel}: \[ arg0 & event \]
\end{avm}
}
%
As discussed in \S\ref{semargsec} above, objects of type {\it event}
bear the feature {\sc tam}.  This means that tense, aspect and mood
information associated with an event are encoded in the {\sc arg0}
of the relation describing that event.  Again, as there is no pointer
to the whole relation that gets passed up to the phrasal level,
this information and
the event index itself should be access via the path {\sc hook.index} 
semantic selection or composition. 

The Matrix provides only one subtype of {\it noun\_rel}, the type {\it
named\_rel}:

\es{
\begin{avm}
{\it named\_rel}: \[ pred & {\bf named\_rel}\\
                    carg & string \]
\end{avm}
}
%
This type is used for proper names (including names of months, days of
the week and days of the month), and it introduces a feature {\sc
carg} (`constant argument') which takes as its value a string
representing the name of the named entity.  It further constraints the
{\sc pred} value to be {\bf named\_rel}.  Thus all proper nouns
contribute relations of the same type and with the same pred value.
Their contribution to the semantics is distinguished solely by their
{\sc carg} (and of course their {\sc arg0}s, which will be distinct for
each proper name in a single sentence).

Note that as {\it named\_rel} is a subtype of {\it noun\_rel}, it
will introduce a referential index ({\it ref-ind}) which must
be bound by some quantifier in order to form a well-formed {\it mrs}.
In the ERG, this is achieved by means of a non-branching rule
which adds the quantifier to a proper noun which usually but not always
lacks an explicit determiner.  Given noun phrases in English like {\it the
younger Smith} and {\it some Roberts}, the ERG defines lexical entries
for proper names as syntactically nouns, not NPs, and employs a specialized
unary syntactic rule to construct a noun phrase from an N-bar headed by
a proper noun (and usually consisting only of that noun).  The Matrix will
support this analysis of proper names, but will of course also enable the
construction of grammars for languages where proper names are simply lexical 
NPs, in which case each lexical entry's semantics will consist of two
relations: a {\it named\_rel} and a {\it quant\_rel} to bind its referential
index.

\subsubsection{{\it arg1\_rel}, {\it arg12\_rel}, ...}

As discussed in \S\ref{arg0sec}, the value of {\sc arg0} is the
distinguished {\it ref-ind} or {\it event} of a relation.  Many
relations, of course, take further arguments.  These are represented
with the features {\sc arg1}, {\sc arg2}, {\sc arg3} and {\sc arg4}.
It is important to note that there is no independent interpretation
of thematic roles attached to these feature names.  That is, {\sc arg1} 
cannot be taken as equivalent to something like {\sc agent} wherever 
it is used.
Rather, we understand the precise interpretation of the role names to
be dependent on the relation they appear in.  This interpretation is
to be specified in a separate component of the grammar called the
Sem-I (`semantic interface'), which provides such information as is
needed to map from {\it mrs}s to application-specific representations.
The Sem-I is further described in \S\ref{semisec} below.

Figure~\ref{arg1234fig} shows the type hierarchy below {\it arg0\_rel},
filling in the information below {\it event\_rel} and {\it arg1\_rel},
which was abbreviated in Figure~\ref{relhier} above.  The features
{\sc arg1} through {\sc arg4} are introduced by the types {\it
arg1\_rel} through {\it arg1234\_rel}, as shown in (\ref{argavm}).
The values of the argument features on these types are only
contrained to be {\it semarg}s, as for any given predicate
they may be {\it individual}s ({\it ref-ind}s or {\it event}s) or
{\it handle}s, in the case of scopal predicates.

\begin{figure}[ht]
\begin{center}

\begin{tabular}{cccccc}
\mc{6}{c}{\node{1}{\it arg0\_rel}}\\[.5cm]

\node{2}{\it adv\_rel} & & \node{4}{\it arg1\_rel} & \node{5}{\it event\_rel} &
\node{6}{\it noun\_rel} & \node{7}{\it quant\_rel}\\[.5cm]

\node{3}{\it neg\_rel} & \node{8}{\it verb\_ellipsis\_rel} & 
\node{9}{\it arg12\_rel} & \node{10}{\it arg1\_ev\_rel} & \node{11}{\it named\_rel} &\\[.5cm]

\node{12}{\it unspec\_compound\_rel} & \node{13}{\it prep\_mod\_rel} &
\node{14}{\it arg123\_rel} & \node{15}{\it arg12\_ev\_rel} & &\\[.5cm]

&& \node{16}{\it arg1234\_rel} & \node{17}{\it arg123\_ev\_rel} & &\\[.5cm]

&&& \node{18}{\it arg1234\_ev\_rel} & &
\end{tabular}

\ncon{1}{2}
\ncon{2}{3}
\ncon{1}{4}
\ncon{1}{5}
\ncon{1}{6}
\ncon{1}{7}
\ncon{4}{8}
\ncon{4}{9}
\ncon{4}{10}
\ncon{5}{10}
\ncon{6}{11}
\ncon{9}{12}
\ncon{9}{13}
\ncon{9}{14}
\ncon{9}{15}
\ncon{10}{15}
\ncon{14}{16}
\ncon{14}{17}
\ncon{15}{17}
\ncon{16}{18}
\ncon{17}{18}

\end{center}
\caption{Hierarchy below {\it arg0\_rel}}
\label{arg1234fig}
\end{figure}

\ees{\label{argavm}
\item \begin{avm}
{\it arg1\_rel}: \[ arg1 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg12\_rel}: \[ arg2 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg123\_rel}: \[ arg3 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg1234\_rel}: \[ arg4 & semarg \]
\end{avm}
}

Note that, in keeping with the strategy for interpretation outlined
above, it is not possible to have an {\sc arg3} without an {\sc arg1}
and an {\sc arg2}.  That is, if a relation takes two arguments 
beyond its distinguished event/index ({\sc arg0}), they will always
be labelled {\sc arg1} and {\sc arg2}.  Furthermore, the {\sc arg1}
should always correspond to the first (least oblique) syntactic
argument, the {\sc arg2} to the second (next least oblique) syntactic
argument, and so on (see \S\ref{linksec} and \citeboth{Fli:Ben:f}).

The type {\it arg1\_rel} does not inherit from {\it event\_rel}, to
allow for the possibility of relational nouns and other relations that
take more than one argument but do not express events.
The types {\it arg1\_ev\_rel} through {\it arg1234\_ev\_rel} provide event
relations with one to four arguments.  Thus (semantically)
intransitive verbs introcue {\it arg1\_rel}s, (semantically) transitive
verbs introduce {\it arg12\_rel}s, etc.  Ordinary nouns introduce {\it
noun\_rel}s, which only have an {\it arg0}.  Relational nouns (such as
{\it picture}) introduce a relation {\it noun\_arg1\_rel} which inherits 
from both {\it noun\_rel} and {\it arg1\_rel} (not {\it
arg1\_ev\_rel}).  A noun with two semantic arguments would require a relation
that inherits from {\it noun\_rel} and {\it arg12\_rel}, and similarly
for a noun with three arguments, if such exist (apart from deverbal nouns,
which are treated for English via lexical rule in the ERG).  These types all 
inherit from {\it arg0\_rel} and
through it {\it relation}, they will all also bear the features {\sc
arg0}, {\sc lbl}, {\sc pred}, and {\sc wlink}.

Note that verbs are not the only lexical entries which introduce subtypes of
{\it event\_rel} in their semantics.  Adjectives can in some languages serve
directly as predicates, be subject to tense and aspect constraints, and take
one or more arguments, all analogous to verbs.  Likewise, prepositions can also
appear as the heads of predicative phrases, so they will also introduce
subtypes of {\it event\_rel} in their semantics.  It is worth noting that
the choice of the name {\it event\_rel} is intended to include states as well
as processes, as required by verbs as well as adjectives and prepositions.
At present the Matrix does not introduce events into the relations for nouns,
though it might be argued that examples in English like {\it the current
president} motivate treating at least some nouns as event-bearing.  Since we
do not know of grammar-internal constraints that require making reference to
events on nouns, we will expect such temporally constrained noun phrases to
be interpreted outside of the grammar.

While verbs identify their {\sc hook.index} value with the {\sc arg0} value in
their main relation, attributive adjectives do not, since they are modifiers, 
which as semantic heads of phrases must expose the index of the noun
that they modify.  So attributive adjectives identify their {\sc hook.index}
value with the {\sc arg1} value in their main relation, with that same value
identified with the value of the {\sc hook.index} in their {\sc mod} attribute,
namely the index of the noun they will modify.  This same value is also
identified with the {\sc xarg} attribute in the adjective's {\sc hook},
necessary if adjectives in a given language can appear in control
constructions.  Predicative adjectives are derived from their attributive
counterparts vie a lexical rule which identifies the {\sc arg0} attribute
of the adjective's main relation with its {\sc hook.index} value, so that the
adjective's event attribute is exposed and can be constrained, for example for
tense and aspect, as in the English example {\it The tree was tall}.

The Matrix anticipates a similar approach for prepositions, treating as basic their
use as (head of phrases) modifying verbal projections, and deriving via lexical
rule their predicative counterparts which expose the event of the preposition
(its main relation's {\sc arg0} value) as its {\sc index} value.  This enables
the desired semantic effects for examples like the English {\it The dog was
in the park}, and for more interesting examples like {\it The dog currently
in the park is barking}.

Adverbs, unlike adjectives and prepositions, do not appear predicatively, and
hence do not need to introduce an independent event.  So they simply identify
the value of their main relation's {\sc arg0} with the index of the phrase
they modify, and further identify this value with their own {\sc hook.index}.

\subsection{Special Relations}

%[FIX - Motivate additional special relations for readability, including
%quant\_rels and coord/subord, but acknowledge that could in principle let
%SemI cope with these just using PRED/SORT/ARGn attributes.]

\subsubsection{Quantifiers}
\label{quantsec}

Quantifiers introduce two scopal features, {\sc rstr} and {\sc body}.
As scopal features, they both take values of type {\it handle}.  The
{\sc rstr} will be related to the top handle of the quantifier's
restriction (i.e., the N$'$ that takes the quantifier as a specifier)
via a {\it qeq} constraint (see \S\ref{qqeq} below for details on
how this constraint is introduced).  The {\sc body} is left unbound:
this is what allows quantifiers to have varied scoping possibilities
(see \citeboth{Cop:Fli:Sag:Pol:03}).  The value of {\sc arg0} is the
referential index that the quantifier binds (see \S\ref{bvsec}).

The type {\it quant\_rel} is thus constrained as follows:

\es{
\begin{avm}
{\it quant\_rel}: \[ arg0 & ref-ind\\
                    rstr & handle\\
                    body & handle \]
\end{avm}
}

Since all quantifiers should need only these features, particular
quantifiers should be distinguished by their {\sc pred} values
(e.g., {\bf \_every\_q\_rel}, {\bf \_some\_q\_rel}, {\bf \_def\_q\_rel}).

\subsubsection{Messages}
\label{msgrelsec}

We provide {\it message} types in the Matrix as an MRS encoding
of the distinctions among types of semantics of clauses in
\citeboth{Gin:Sag:00}.  Clauses can express
commands, questions, or propositions.  The Matrix
organizes the subtypes of {\it message} as shown in Figure~\ref{msgfig}.

\begin{figure}
\begin{center}

{\it
\begin{tree}
\br{basic\_message}{\lf{no-msg}
\br{message}{\lf{command}
             \br{\ \ \ prop-or-ques}{
                               \br{abstr-ques}{\lf{question}
                                               \lf{\ \ \ \ \ \ \ \ \ ne\_rel}}
                               \lf{\ \ \ \ \ \ \ \ \ \ proposition}}}}
\end{tree}
}

\end{center}
\caption{Subhierarchy under {\it message}}
\label{msgfig}
\end{figure}

This type hierarchy has a bit more structure than might otherwise be
expected as we anticipate that the {\it messsage} type of a clause (through
{\sc keys.message}) will used in semantic selection by
clause-taking verbs.  As some verbs (like {\it know}) accept {\it
proposition} or {\it question} but not {\it command} complements, we
provide the intermediate type {\it prop-or-ques}.
Among questions ({\it abstr-ques}), we distinguish between
ordinary {\it questions} and the type of confirmation-seeking
question expressed by a tag question in English, the particle {\it ne}
in Japanese, the particle {\it ne} in German, or the equivalent in
other languages.\fn{On {\it ne\_rel}, see \citeboth{Ben:Fli:99}.}

The ERG has adopted the further assumption that all questions contain an
immediately embedded proposition, motivated both by (reasonable but
controversial) hypotheses about inference in discourse, and by concerns for
simplicity in the hierarchy of constructions for English.  This choice of
implementation for the semantics of questions is supported by the Matrix, but 
not required by it.

\subsubsection{Subordination and Coordination}
\label{coordsec}

The Matrix provides two relation types for subordination and coordination. 
The first, the more general type {\it subord\_or\_conj\_rel}, is used directly
by subordinating conjunctions like English {\it since}, and the second is a 
subtype {\it conjunction\_rel} used by ordinary conjunctions like English 
{\it and}.  Both coordination and subordination relations take two handles as 
arguments; in the
case of subordination, these correspond to the {\sc ltop} values of
the main and subordinate clauses, while for coordination they correspond to
the {\sc ltop} values of the two conjuncts.  These
are the values of the features {\sc l-hndl} and {\sc r-hndl}, with the names
chosen to encode explicitly the order of left and right conjuncts,
since this order can be semantically (or pragmatically) relevant at least for
coordination.

\es{
\begin{avm}
{\it subord\_or\_conj\_rel}: \[ l-hndl & handle\\
                                r-hndl & handle \]
\end{avm}
}

The main relation introduced by specific subordinating conjunctions (e.g., 
{\it if}, {\it because}, {\it while}) will be simply the general relation
{\it subord\_or\_conj\_rel}, with distinguishing values provided by
each lexical entry for the {\sc pred} value.  There
is, however, more to be said about coordination.  Coordination of clauses,
predicates or noun phrases requires the construction of a single {\it
individual} ({\it event} or {\it ref-ind}) that can be the value of a
role feature in any outside predicate that takes the conjoined
entity as an argument.  This is achieved by giving {\it conjunction\_rel}
the three additional features shown in (\ref{conjrel}):

\es{\label{conjrel}
\begin{avm}
\[ c-arg & conj-ind\\
   l-index & index\\ 
   r-index & index \]
\end{avm}
}
%
The value of {\sc c-arg} is the conjoined index ({\it conj-ind}) which
serves as a pointer to the separate conjoined entity.  

Of course, it's possible to conjoin more than two noun phrases,
predicates or clauses.  In order to keep a determinate number of
features for any given relation, we represent multi-coordination via a
series of chained {\it conjunction\_rel}s, where the {\sc r-index} of
all but the lowest {\it conjunction\_rel} is the {\sc c-arg} of the next
{\it conjunction\_rel}.
%Is this right?  What about the {\sc
%r-hndl}?  Provide example? [DPF] Yes, right, and for non-NP signs, also
%true for r-hndl.  Yes, we should provide an example.

Note that in the coordination of noun phrases, the {\sc l-hndl} and 
{\sc r-hndl} values of the {\it conjunction\_rel} will be identified with the
{\sc ltop} values of each of the two noun phrases, but these handle values
will not be identified with the {\sc lbl} values of any relations in the
phrase, given our treatment of quantifiers as underspecified with respect to
scope.  In all other cases of coordination (clauses, VPs, predicative phrases,
etc.), the values of {\sc l-hndl} and {\sc r-hndl} will be identified with
the {\sc lbl} values of relations in the phrase.


\subsubsection{Miscellaneous Special Relations}

%[Discuss at least {\it unspec\_compound\_rel} and
%{\it verb\_ellipsis\_rel}.  It's unclear to me at this point
%why these merit their own types, if we're going to go the
%{\sc pred} route].
%FIX - yes, just use PRED, not new relations.  But discuss and illustrate,
%maybe under banner of construction-specific relations ...

\subsubsection{User Defined Special Relations}

%[ ... Give guideance here on what to do in case a new relation type
%seems merited.  Discuss cases like number names or numeral classifiers? ... ]
%FIX - Maybe one or two examples.

\section{Features of Indices: Agreement and TAM}
\label{AgrTAM}

The Matrix currently provides only minimal support for more fine-grained
constraints on semantic indices, in part due to the high degree of variation
needed across grammars to encode constraints on attributes like person, number,
and gender for referential indices, or tense, aspect, and mood for events.
Even a seemingly uncontroversial step like introducing separate attributes for 
each of these properties on indices has proven to be unproductive, since
a given language may conflate two such attributes morphologically and
syntactically.

For example, the ERG captures the distribution of person and number in English
by introducing types which conflate these two properties, enabling a direct
expression of the relevant syntactic contrasts without requiring a disjunctive
representation of the constraints on subject-verb agreement.  The ERG 
introduces an attribute of the type {\it png} called {\sc pn} (person-number),
whose values are of the type {\it pernum} defined as in Figure~\ref{pernumfig}.
Given these types, the constraint on the {\sc agr} value for
non-third-singular verbs is simply\\

\es{
\begin{avm}
\[ {\it non3sg-verb} \\
   {\sc agr} & \[ {\sc pn} & {\it non3sg} \] \]
\end{avm}\\
}

\noindent
since the type {\it non3sg} will unify with any of its subtypes, including the
{\sc agr} values {\it 1sg, 2sg, 1pl, 2pl}, and {\it 3pl}.


\begin{figure}[ht]
\begin{center}
{\setlength{\tabcolsep}{1mm}
\begin{tabular}[t]{ccccc}
\multicolumn{5}{c}{\node{pernum}{\it pernum}}\\[.2in]
\multicolumn{2}{c}{\node{13sg}{\it 1or3sg}} & \multicolumn{3}{c}{\node{non3}{\it non3sg}}\\[.2in]
\node{3sg}{\it 3sg} & \node{1sg}{\it 1sg} & \multicolumn{3}{c}{\node{non1}{\it non1sg}}\\[.2in]
& \multicolumn{2}{c}{\node{2per}{\it 2per}} & \node{1pl}{\it 1pl} & \node{3pl}{\it 3pl}\\[.2in]
& \node{2sg}{\it 2sg} & \node{2pl}{\it 2pl} &&\\[.2in]
\end{tabular}
}
\nodeconnect{pernum}{13sg}
\nodeconnect{pernum}{non3}
\nodeconnect{13sg}{3sg} 
\nodeconnect{13sg}{1sg} 
\nodeconnect{non3}{1sg} 
\nodeconnect{non3}{non1} 
\nodeconnect{non1}{2per}
\nodeconnect{non1}{1pl}
\nodeconnect{non1}{3pl}
\nodeconnect{2per}{2sg}
\nodeconnect{2per}{2pl}
\end{center}

\caption{Hierarchy of {\sc pn} values in the ERG}
\label{pernumfig}
\end{figure}

This generalization would be difficult to express without disjunctions if
person and number were distinct attributes, and we expect similar conflations
in particular languages for other agreement attributes, so we have left the
internal structure of the values for the attributes {\sc png} and {\sc tam}
unspecified in the Matrix.  In future work, we hope to include possible
tense/aspect systems as modules in the Matrix, i.e., as components that we 
expect to be useful in many but not all languages.


\section{Syntax-Semantics Interface}
\label{synsem-i}

Section \ref{basicobj} discussed the basic semantic objects defined
in the Matrix.  Section \ref{relarg} discussed types of relations
and the representation of arguments of relations.  The preceding
Section described our approach to representing agreement and TAM.
This should give a fairly complete picture of the kinds of semantic
representations Matrix-derived grammars should build.  This section
addresses how to go about building those representations
compositionally as words are combined into successively larger phrases
in the syntax.  This section is organized as follows:
\S\ref{semprinsec} describes the implementation of general semantic
principles. \S\ref{semheadsec} describes the identification of
semantic heads.  \S\ref{ccontsec} discusses constructions and lexical
rules that make semantic contributions beyond simply combining the
semantics of their daughters.  \S\ref{linksec} describes the linking
of syntactic and semantic arguments within the lexicon, and the
role of syntactic rules in associating the indices of syntactic
arguments with the appropriate semantic requirements of a predicate.
\S\ref{bvsec} describes how to ensure that all nominal indices are
bound by an appropriate quantifier.  \S\ref{hconssec} provides
examples of handle constraints, including constraints on the {\sc
rstr} values of quantifiers, how messages interact with handle
constraints, and handle constraints concerning scopal modifiers like
{\it probably}.  \S\ref{semselkey} describes how the value of {\sc
keys} can be used to implement semantic selectional restrictions.
Finally, \S\ref{xargsec} describes mismatches between syntax and
semantics, including control constructions, expletives, and
raising of {\sc key} and {\sc index} values (e.g., by the English auxiliary 
{\it do}).

\subsection{Semantic Principles}
\label{semprinsec}

With every word or phrase providing a semantics which consists of {\sc
hook}, {\sc rels}, and {\sc hcons}, the principles of semantic
composition in phrase structure rules can be stated (and implemented)
quite elegantly, following the definitions in \citeboth{Cop:Las:Fli:01}:

\begin{enumerate}
\item The value for {\sc rels} on the mother of a phrase is the result of
appending the {\sc rels} values of all of its daughters.
\item The value for {\sc hcons} on the mother of a phrase is the result of
appending the {\sc hcons} values of all of its daughters.
\item The value for {\sc hook} on the mother of phrase is identified with
the {\sc hook} value of its semantic head daughter, where each phrase type
uniquely determines which of the daughters is the semantic head.
\end{enumerate}

In the Matrix, principles 1 and 2 are
implemented as constraints on a few high-level types ({\it lex-rule},
{\it basic-unary-phrase} and {\it basic-binary-phrase}) within the
{\it sign} subhierarchy (sketched in Figure~\ref{signhier}), such that they
are inherited by all phrases and lexical rules.  In addition, the type
{\it phrase-or-lexrule} identifies the mother's {\sc hook} with the
{\sc hook} of the semantics provided by the rule itself (the value of
a feature called {\sc c-cont}; see \S\ref{ccontsec} below).  More
specialized phrase types identify the {\sc hook} of the {\sc c-cont}
with the {\sc hook} of the head or non-head daughter, as appropriate.

\begin{figure}[ht]
\begin{center}
%\begin{small}
\hspace{-20pt}\begin{tabular}[t]{ccccccc}
\mc{7}{c}{\node{1}{\it sign}}\\[.5cm]
\mc{3}{c}{\node{2}{\it word-or-lexrule}} & \mc{4}{c}{\node{3}{\it phrase-or-lexrule}}\\[.5cm]
\node{4}{\it word} & &\node{5}{\it lex-rule} & & \node{6}{\it phrase}\\[.5cm]
&&\node{7}{\ldots} & \node{8}{\it basic-} & \node{9}{\it basic-} & \node{10}{\ldots}\\
&&&{\it unary-} & {\it binary-}\\
&&&{\it phrase} & {\it phrase}\\
\end{tabular}
\nodeconnect{1}{2}
\nodeconnect{1}{3}
\nodeconnect{2}{4}
\nodeconnect{2}{5}
\nodeconnect{3}{5}
\nodeconnect{3}{6}
\nodetriangle{5}{7}
\nodeconnect{6}{8}
\nodeconnect{6}{9}
\nodeconnect{6}{10}
%\end{small}
\end{center}
\caption{Subhierarchy under {\it sign}}
\label{signhier}
\end{figure}

Principles 1 and 2 require the monotonic accumulation of {\sc rels} and {\sc
hcons} values from daughter to mother in a phrase.  The values of
these features are implemented as difference lists (typed feature
structures which bear values for two attributes {\sc list} and {\sc
last}), which allows us to state the accumulation of values using the
same single operation of unification of typed feature structures.
(\ref{dla}) shows the constraints on the type {\it
basic-unary-phrase}, including the `diff-list appends' that implement
principles 1 and 2.

\enumsentence{\label{dla}
\begin{avm}
{\it basic-unary-phrase}: \[ \avmspan{synsem.local.cont \[ rels & \[ list & \@1\\
        		            last & \@2 \]\\
			  hcons & \[ list & \@4 \\
				     last & \@5 \]\]}\\
   c-cont & \[ rels & \[ list & \@3\\
		         last & \@2 \]\\
	      hcons & \[ list & \@6\\
		         last & \@5 \]\]\\
   \avmspan{args \< \[ \ldots \[ cont \[ rels & \[ list & \@1\\
						   last & \@3 \]\\
					 hcons & \[ list & \@4 \\
						     last & \@6 \]\]\]\] \>} \]
\end{avm}
}

\subsection{{\sc hook} Features and Semantic Heads}
\label{semheadsec}

The type {\it head-compositional} (a subtype of {\it headed-phrase})
provides the constraint that identifies the {\sc hook} of the {\sc
c-cont} (and therefore also of the mother) with the {\sc hook} of the
head-daughter:

\es{
\begin{avm}
{\it head-compositional}: \[ c-cont.hook & \@1\\
                             head-dtr & \[ synsem.local.cont.hook & \@1 \]\]
\end{avm}
}
%
The types {\it basic-head-subj-phrase}, {\it basic-head-comp-phrase},
{\it basic-extracted-comp-phrase}, and {\it
basic-head-opt-comp-phrase} (and therefore its two subtypes)
all inherit this constraint from {\it head-compositional}.  That is,
these types of phrases are all cases where the syntactic head and the
semantic head are the same.   Note that {\it basic-extracted-comp-phrase} 
and {\it basic-head-opt-comp-phrase} are unary phrases, but it is still
necessary to determine whether or not they are head-compositional, as
there are also unary phrases which contribute constructional semantics
and therefore do not identify the {\sc hook} of the {\sc c-cont} with the
{\sc hook} of the daughter (see \S\ref{ccontsec} below). 

The type {\it head-mod-phrase} does not inherit from {\it
head-compositional}, nor do any of its subyptes.  Indeed, {\it
basic-head-mod-phrase-simple} imposes the opposite constraint,
identifying the {\sc hook} values of the {\sc c-cont} and the non-head
daughter.
%We need to include a longer description of how scopal and
%intersective modification work.  Where would that best fit in? [DPF] Section
%6.6.3, which might be retitled Modification  
Thus we treat the modifier in head-modifier constructions as the semantic
head, even though it is not the syntactic head.  Similarly, in {\it
basic-head-spec-phrase} the {\sc hook} of the {\sc c-cont} is
identified with the {\sc hook} of the specifier daughter, as we take
specifiers to be semantic heads.
%For some reason this constraint is
%actually implemented via the {\sc spr} feature of the head-daughter,
%which seems unnecessarily baroque to me.  Should we fiques it?
%It looks like {\it basic-head-filler-phrase} doesn't
%identify any semantic head.  This probably needs to be fixed, too. [DPF] 
%Agreed.  Probably want a type nonhead-compositional for spr-hd and head-adj.

We have (syntactically) headed phrases where the semantic head is
the syntactic head, headed phrases where the semantic head is instead
the (syntactic) non-head daughter, and headed phrases where the {\sc
c-cont} is the semantic head.  The Matrix does not provide any
subtypes of {\it non-headed-phrase}, but every phrase must have
some semantic head, even if only the {\sc c-cont}.  The constraints
on {\it lex-rule}, {\it basic-binary-phrase}, and {\it basic-unary-phrase}
ensure that the {\sc hook} of the mother will always be identified
with the {\sc hook} of the {\sc c-cont}, but it is the responsibility
of the grammar developer to make sure that the {\sc hook} of the
{\sc c-cont} provides sufficient information.  This will happen if
it is identified with the {\sc hook} of a daughter (as in
the constructions discussed above), or if the {\sc c-cont} has
a non-empty {\sc rels} value and the features inside {\sc hook}
are related to the appropriate parts of some {\it relation} on 
{\sc rels}.

The next subsection provides some examples of semantically
contentful constructions.

\subsection{Semantic Contributions of Constructions}
\label{ccontsec}

Since some phrase types may introduce semantic content which is not
drawn from any of the daughters of the phrase, the MRS framework
provides an attribute for phrasal signs called {\sc c-cont} (for
construction content), which behaves with respect to the semantics
principles just as though it were another daughter of the phrase (see,
for example, (\ref{dla}) above).  {\sc c-cont} is implemented in the
Matrix as a top-level attribute of phrases and lexical rules,
introduced on the type {\it phrase-or-lexrule}.  Like {\sc cont}, its
value is of type {\it mrs}.  

If a phrase does not introduce any additional semantic content of its
own, the values for the attributes {\sc rels} and {\sc hcons} in {\sc
c-cont} will be empty lists, so unary and binary phrases can safely
always append these values to those supplied by the syntactic
daughters.  Likewise, the {\sc hook} value of a phrase is always
identified with its {\sc c-cont}'s value for {\sc hook}, where for
most phrases this {\sc hook} in {\sc c-cont} will simply be identified
with that of one of the daughters of the phrase, namely the semantic
head daughter.

\subsubsection{Syntactic Constructions}

One example of a phrase type in the Matrix that does introduce its own
semantic content is the type for (non-relative) clauses ({\it
non-rel-clause}), which introduces a relation encoding the
illocutionary force of the clause.  Such relations (e.g., {\it
prpstn\_m\_rel}, mentioned above) are of type {\it message}, following the
analysis in \cite{Gin:Sag:00}.

We illustrate construction-introduced semantic content further with
the treatment of noun-noun compounds in the ERG.  In the analysis of the sentence {\it the office chair
arrived}, the phrase {\it office chair} is built using a syntactic
rule specifically for noun-noun compounds, and this rule introduces a
generic two-place relation {\bf n-n-cmpnd\_rel} which relates the variables
introduced by the two nouns.  The syntactic structure is sketched in
(\ref{compound}), where the head-specifier rule is used to combine the
determiner and the compound noun, while the head-subject rule combines
the full NP with the verb phrase {\it arrived}.  The corresponding MRS
semantics is shown in (\ref{compsem}):

\enumsentence{\label{compound}
\begin{tree}\psset{treefit=tight}
\br{S}{\br{NP}{\br{Det}{{\lf{the}}}
               \br{N}{\br{N}{\lf{office}}
                      \br{N}{\lf{chair}}}}
       \br{VP}{\br{V}{\lf{arrived}}}}
\end{tree}\\
\\
The office chair arrived
}

\enumsentence{\label{compsem}
\begin{avm}
\[ {\it mrs}\\
hook & \[ ltop & h1\\
          index & e2 \]\\
rels & \avml\q<\tn{!} \[ \avmspan{\bf prpstn\_m\_rel}\\
              lbl & h1\\ 
              marg & h4 \],
            \[ \avmspan{\bf def\_q\_rel}\\
               lbl & h10\\
               arg0 & x8\\
               rstr & h11\\
               body & h12 \],
            \[ \avmspan{\bf \_chair\_n\_rel}\\
               lbl & h7\\
               arg0 & x8 \],
            \[ \avmspan{\bf \_office\_n\_rel}\\
               lbl & h14\\
               arg0 & x16 \], \\
            \[ \avmspan{\bf udef\_q\_rel}\\
               lbl & h17\\
               arg0 & x16\\
               rstr & h18\\
               body & h19 \],
            \[ \avmspan{\bf n-n-cmpnd\_rel}\\
               lbl & h7\\
               arg1 & x16\\
               arg2 & x8 \],
            \[ \avmspan{\bf \_arrive\_v\_rel}\\
               lbl & h21\\ 
               arg0 & e2\\
               arg1 & x8 \] \tn{!}\q>\avmr\\
hcons & \q<\tn{!} \[ {\it qeq}\\ 
                     HARG & h4\\ 
                     LARG & h21 \],
                  \[ {\it qeq}\\
                     HARG & h11\\
                     LARG & h7 \],
                  \[ {\it qeq}\\
                     HARG & h18\\
                     LARG & h14 \]\ \tn{!}\q> \]
\end{avm}

%% $\avmplus{\att{mrs}\\
%%           \attval{HOOK}{\avmplus{\attval{LTOP}{h1}\\
%%                                  \attval{INDEX}{e2}}}\\
%%           \attvallist{RELS}{
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h1}\\
%%          \attvaltyp{MARG}{h4}},
%% \avmplus{\att{def}\\
%%          \attvaltyp{LBL}{h10}\\
%%          \attvaltyp{ARG0}{x8}\\
%%          \attvaltyp{RSTR}{h11}\\
%%          \attvaltyp{BODY}{h12}},
%% \avmplus{\att{chair}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG0}{x8}},
%% \avmplus{\att{office}\\
%%          \attvaltyp{LBL}{h14}\\
%%          \attvaltyp{ARG0}{x16}},
%% \avmplus{\att{udef}\\
%%          \attvaltyp{LBL}{h17}\\
%%          \attvaltyp{ARG0}{x16}\\
%%          \attvaltyp{RSTR}{h18}\\
%%          \attvaltyp{BODY}{h19}},\\
%% \avmplus{\att{n-n-cmpnd}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG1}{x16}\\
%%          \attvaltyp{ARG2}{x8}},
%% \avmplus{\att{arrive}\\
%%          \attvaltyp{LBL}{h21}\\
%%          \attvaltyp{ARG0}{e2}\\
%%          \attvaltyp{ARG1}{x8}}}\\
%% \attvallist{HCONS}{
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h4}\\
%%              \attval{LARG}{h21}},
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h11}\\
%%              \attval{LARG}{h7}},
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h18}\\
%%              \attval{LARG}{h14}}}}$
}

This MRS representation contains two noun relations each associated with
a quantifier relation to bind the variables that are their {\sc arg0} values.
The two noun variables are also identified with the 
{\sc arg1} and {\sc arg2} attributes of the {\bf n-n-cmpnd\_rel} relation, and
the variable for the head noun {\it chair} is also the value of the single
argument of the {\it arrive} relation.  The {\it n-n-cmpnd} relation is 
introduced by the grammar in the {\sc rels} attribute of the {\sc c-cont} 
of the grammar rule for noun-noun compounds, which also identifies the 
assignments of the two nominal instance variables (supplied by its two
daughters) to the {\sc arg0} and {\sc arg1} attributes of that {\it n-n-cmpnd}
relation.  The relevant constraint on the grammar rule is sketched in
(\ref{nnc}):

\enumsentence{\label{nnc}
\scriptsize
\begin{avm}
  \[ head-dtr...hook & \[ index \@1 \]\\
    non-head-dtr...hook & \[ index \@2 \]\\
    \avmspan{c-cont \[ rels.list \< \[ \avmspan{\it n-n-cmpnd}\\
				  arg1 & \@1\\
                                  arg2 & \@2 \] \> \]}\]
\end{avm}
}

As discussed above, general principles of semantic composition that
are encoded in the Matrix types ensure that rule-specific relations
are gathered up along with the relations supplied by the daughters of
the rule, and that the appropriate external semantic hooks (the {\sc
ltop} and {\sc index} values) are identified on the phrase itself,
ready for further composition.


\subsubsection{Lexical Rules}

Lexical rules are treated in the Matrix as a particular type of unary
rule, in most respects like syntactic unary rules, though lexical
rules are prevented from interleaving with syntactic rules.  Thus
semantic composition for lexical rules is implemented using the same
principles as outlined above for syntactic phrases.  A lexical rule
may or may not introduce semantic content of its own; if it does, that
content is found in the {\sc c-cont} attribute of the rule and is
combined with the semantic content of the (single) daughter (the
`input' to the lexical rule) by those same principles.

Note that this approach imposes a strong constraint on the
directionality of lexical rules in the Matrix, since the principles of
composition guarantee monotonic accumulation of atomic predications,
so no semantic content from a daughter in a phrase or lexical rule is
ever lost.  For example, a lexical rule relating the
causative/inchoative alternation in English for verbs like {\it break}
will have to treat the inchoative lexical entry as the `input' to the
lexical rule (that is, the daughter), and the semantically richer
causative lexical entry as the `output' (the {\sc synsem} value of the
lexical rule).


\subsection{Linking}
\label{linksec}

Lexical entries can select syntactic arguments, such as complements, subjects,
and specifiers, constraining the properties of these arguments to capture the
relevant subcategorization requirements.  At the same time, these lexical
entries can impose constraints which determine the way that the semantics of 
their arguments will combine with the semantics of the selecting entry.
These constraints linking the semantic hooks of syntactic arguments to the
appropriate semantic argument positions are introduced in lexical entries
and interact with corresponding constraints in the constructions
that combine a word or phrase with one or more of its syntactic arguments.

For example, a transitive verb like English {\it chase} subcategorizes for an 
NP subject and an NP object, where in the ERG the verb combines with its 
object using the {\it head-complement} rule, and with its subject using the
{\it head-subject} rule.  The semantic index of the subject NP is assigned
to the {\sc arg1} role in the {\bf \_chase\_v\_rel} relation, while the index of
the object NP is assigned to the {\sc arg2} role, where the interpretation
of these role names (as the chaser and the thing chased, respectively) is
provided by a separate component of the grammar called the Sem-I, described
in \S\ref{semisec}.

The linking type for transitive verbs like {\it chase} in the ERG includes the 
following simple linking constraints:\fn{These \label{argsfn}constraints are stated
on the type {\it trans-lt}, but rather inherited from its supertypes.  We
display them on {\it trans-lt} for expository convenience.

In future versions of the Matrix,
we expect to state such constraints using the feature ARG-S (Argument Structure)rather than valence features like SUBJ or COMPS.}

\es{
\begin{avm}
\[ {\it trans-lt}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.index & \@2 \] \q>\\
   {-}{-}keyrel  & \[ arg1 & \@1 \\
            arg2 & \@2 \] \]
\end{avm}
}

%% $\avmplus{\att{\it transitive-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{2}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}
%%                   \attval{ARG2}{\ind{2}}}}}$\\

When the verb phrase {\it chased the cat} is constructed using the
{\it head-complement} rule, the feature structure for the sign {\it the cat}
is unified with the constraints in the {\sc comps} attribute of {\it chase},
including both the syntactic requirements for an accusative NP and the
semantic constraints which identify the semantic index of that NP with the
{\sc arg2} role in the head's relation {\bf \_chase\_v\_rel}.  An analogous
identification is made when tne subject NP {\it the dog} is combined with
the verb phrase {\it chased the cat} using the {\it subject-head} rule, so
that the NP's semantic index is unified with the {\sc arg1} role of the
{\bf \_chase\_v\_rel}.

Lexical entries like the English verb {\it insist} which take sentential
complements differ from simple transitive verbs in that they impose a 
linking constraint on the local top {\sc ltop} attribute of their complement 
rather than on the {\sc index} of that complement.  This ensures that the
semantics of the embedded sentence falls within the scope of the semantic
relation introduced by the selecting verb, as discussed in \S\ref{msgsec} 
below.

The lexical type for verbs like {\it insist} as in {\it Kim insisted that
Sandy was right} in the ERG includes the following linking constraints:\fn{See note \ref{argsfn}.}

\es{
\begin{avm}
\[ {\it cp-intrans-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.ltop & \@2 \] \q>\\
   {-}{-}keyrel & \[ arg1 & \@1\\
            arg2 & \@2 \] \]
\end{avm}
}

%% $\avmplus{\att{\it s-comp-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.LTOP}{\ind{2}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}
%%                    \attval{ARG2}{\ind{2}}}}}$\\

Lexical entries for modifiers introduce similar linking constraints
on the words or phrases that they modify, even though modifiers are not the
syntactic heads in head-modifier constructions.  These constraints on both
the syntax and semantics of modified phrases are introduced in the 
{\sc head.mod} attribute of a modifier's lexical entry (instead of the
valence features including {\sc subj} or {\sc comps}), and the constructions
that combine modifiers and heads unify the feature structure of the head
with the constraints in the {\sc head.mod} attribute of the modifier, analogous
to the effects of the valence-combining constructions.

For example, the lexical type for simple intersective adjectives like English
{\it tall} includes the following linking constraints:

\es{
\begin{avm}
\[ {\it adj-synsem}\\
   head.mod & \q< \[ hook.index & \@1 \] \q>\\
   {-}{-}keyrel & \[ arg1 & \@1 \]\]
\end{avm}
}

%% $\avmplus{\att{\it intersect-adj}\\
%%           \attvallist{HEAD.MOD}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}}}}$\\

\noindent
Then the {\it intersective-modifier-head} rule which combines {\it tall} with 
{\it chair} unifies the {\sc head.mod} constraints of {\it tall} with the 
feature structure for {\it chair}, and in addition the rule identifies the
{\sc ltop} values of the two daughters, ensuring that the noun and its
modifying adjective are assigned the same scope since they have a common
handle.

Scopal modifiers like the English adverb {\it probably} impose a linking
constraint on the phrase they modify which is analogous to that introduced
by verbs like {\it insist}, making reference to the modified phrase's 
{\sc ltop} attribute rather than its {\sc index}:

\es{
\begin{avm}
\[ {\it basic\_scopal\_adverb\_synsem}\\
  head.mod & \q< \[ hook.ltop & \@1 \] \q>\\ 
  {-}{-}keyrel & \[ arg1 & \@1 \] \]
\end{avm}
}

%% $\avmplus{\att{\it scopal-adv}\\
%%           \attvallist{HEAD.MOD}{
%%           \avmplus{\attval{HOOK.LTOP}{\ind{1}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}}}}$\\

The construction that combines scopal modifiers with their heads will unify
this constraint from the modifier on the head's {\sc ltop}, resulting in the
semantics of the modified phrase falling within the scope of the relation
introduced by the modifier, as desired.

\subsection{Indices Bound by Quantifiers}
\label{bvsec}

\subsection{Imposing Handle Constraints}
\label{hconssec}

\subsubsection{Embedded Clauses}
\label{msgsec}

%[ ... Includes discussion of messages ... ]

\subsubsection{Quantifiers}
\label{qqeq}
\subsubsection{Scopal Modifiers}

%[ ... Include discussion of LKB's MRS scoping machinery and
%constraints on well-formed MRSs. ... ]
 
\subsection{Semantic Selection: Keys}
\label{semselkey}

%% [DAN: Can you provide a few more examples of how {\sc key} and {\sc
%% altkey} are used in the ERG?  I think that I used one or the other as
%% a junk slot to hide a relation inside numeral classifiers that was
%% added to {\sc rels} just in case they were used as nominal heads (via
%% a lexical rule), but I'm not sure this is the kind of trick we should
%% be encouraging...  It looks like ALTKEY at least seems to be used by
%% some constructions in the ERG to introduce relations --
%% e.g. proper\_np\_phrase.  Maybe explain that? Discuss how we use keys
%% in the tags paper, or just ref to it?]

\subsection{Syntax-Semantics Mismatches}
\label{xargsec}

When defining the linking between syntactic arguments of a lexical entry
and the corresponding semantic argument positions in the relation introduced
by that entry, a grammar writer will often encounter mismatches.  A single
syntactic argument may have its index be linked to 
%more than one position in
%the head's semantics 
to a position in more than one head's semantics
(as in equi constructions), or it may not appear in
the head's semantic relation at all, as in raising or expletive constructions.
In this section we describe the mechanisms provided in the Matrix for 
defining these mismatches in the syntax-semantics interface.

\subsubsection{External arguments and control}

We have made reference so far to two of the {\it hook} attributes,
{\sc ltop} and {\sc index}, both of which play a crucial role in the semantic
construction of every phrase.  A third attribute, {\sc xarg}, is relevant for
control phenomena such as equi and raising, since it identifies the semantic
index of a phrase's external argument (usually the subject of a verb phrase).
Identifying this property of a phrase as part of the hook allows our general
principles of semantic composition to make this attribute visible for control
of subject-unsaturated complements (VPs, predicative PPs, etc.) and also for
agreement even at the sentence level, as for example in tag questions in
English \cite{Ben:Fli:99}.  An example using this {\sc xarg}
attribute in composition is given in (\ref{control}), with the lexical type
for subject-equi verbs given in (\ref{seq}).

\enumsentence{\label{control}
The dog tried to bark.
}

\begin{tree}
\psset{levelsep=.7in}
\br{S}{
       \br{\begin{avm}\[index & \@1\]\end{avm}}{
              \br{Det}{\lf{the}}
              \br{\begin{avm}\[index & \@1\\ arg0 & \@1 \]\end{avm}}{\lf{dog}}}
       \br{\begin{avm}\[subj & \q< \[ index & \@1 \] \q>\]\end{avm}}{
              \br{V}{\lf{tried}}
              \br{\begin{avm}\[xarg & \@1 \]\end{avm}}{
                     \br{C}{\lf{to}}
                     \br{\begin{avm}\[xarg & \@1\\ arg1 & \@1 \]\end{avm}}{\lf{bark}}}}}
\end{tree}

%% \br{S}{\br{$\avmplus{\attval{INDEX}{\ind{1}}}$}
%%            {\br{Det}{\lf{the}}
%%             \br{$\avmplus{\attval{INDEX}{\ind{1}}\\
%%                           \attval{ARG0}{\ind{1}}}$}{\lf{dog}}}
%%        \br{$\avmplus{\attval{S.INDEX}{\ind{1}}}$}
%%            {\br{V}{\lf{tried}}
%%             \br{$\avmplus{\attval{XARG}{\ind{1}}}$}
%%                 {\br{C}{\lf{to}}
%%                  \br{$\avmplus{\attval{XARG}{\ind{1}}\\
%%                                \attval{ARG1}{\ind{1}}}$}{\lf{bark}}}}}


\enumsentence{\label{seq}
Type for subject-equi verbs like {\it try}

\begin{avm}
\[ {\it subj-equi-verb}
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.xarg & \@1 \] \q>\\
   {-}{-}keyrel & \[ arg1 & \@1 \] \]
\end{avm}

}

%% $\avmplus{\att{\it subj-equi-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.XARG}{\ind{1}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}}}}$


Here the lexical entry for the verb {\it try} identifies its VP complement's
semantic external argument ({\sc xarg} value) with its subject's semantic index
({\sc index} value), and further identifies that index with the appropriate
role (the {\sc arg1}) in the lexical relation introduced by the verb.  The MRS 
semantics constructed for this example is given in (\ref{csem}).


\enumsentence{\label{csem}
\begin{avm}
\[ {\it mrs}\\
   hook & \[ ltop & h1\\
             index & e2 \]\\
   rels & \avml\q<\tn{!} \[ {\bf prpstn\_m\_rel}\\
                       lbl & h1\\
                       marg & h4 \] ,
                    \[ {\bf def\_q\_rel}\\
                       lbl & h10\\
                       arg0 & x8\\
                       rstr & h11\\
                       body & h12 \] ,
                    \[ {\bf \_dog\_n\_rel}\\
                       lbl & h7\\
                       arg0 & x8 \] ,\\
                    \[ {\bf \_try\_v\_rel}\\
                       lbl & h21\\
                       arg0 & e2\\
                       arg1 & x8\\
                       arg2 & h22 \] ,
                    \[ {\bf prpstn\_m\_rel}\\
		       lbl & h22\\
                       marg & h23 \] ,
		    \[ {\bf \_bark\_v\_rel}\\
                       lbl & h24\\
                       arg0 & e3\\
                       arg1 & x8 \]\ \tn{!}\q>\avmr\\
  hcons & \q<\tn{!} \[ {\it qeq}\\
                       harg & h4\\
                       larg & h21\] ,
		    \[ {\it qeq}\\
		       harg & h11\\
		       larg & h7 \] ,
		    \[ {\it qeq}\\
		       harg & h23\\
		       larg & h24 \]\ \tn{!}\q> \]
\end{avm}
}


%% \noindent
%% {\small
%% $\avmplus{\att{mrs}\\
%%           \attval{HOOK}{\avmplus{\attvaltyp{LTOP}{h1}\\
%%                                  \attvaltyp{INDEX}{e2}}}\\
%%           \attvallist{RELS}{
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h1}\\
%%          \attvaltyp{MARG}{h4}},
%% \avmplus{\att{def}\\
%%          \attvaltyp{LBL}{h10}\\
%%          \attvaltyp{ARG0}{x8}\\
%%          \attvaltyp{RSTR}{h11}\\
%%          \attvaltyp{BODY}{h12}},
%% \avmplus{\att{dog}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG0}{x8}},
%% \avmplus{\att{try}\\
%%          \attvaltyp{LBL}{h21}\\
%%          \attvaltyp{ARG0}{e2}\\
%%          \attvaltyp{ARG1}{x8}\\
%%          \attvaltyp{ARG2}{h22}},
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h22}\\
%%          \attvaltyp{MARG}{h23}},
%% \avmplus{\att{bark}\\
%%          \attvaltyp{LBL}{h24}\\
%%          \attvaltyp{ARG0}{e3}\\
%%          \attvaltyp{ARG1}{x8}}}\\
%% \attvallist{HCONS}{
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h4}\\
%%              \attvaltyp{LARG}{h21}},
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h11}\\
%%              \attvaltyp{LARG}{h7}},
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h23}\\
%%              \attvaltyp{LARG}{h24}}}}$}
%% }

The construction of this representation is the result of the same general
principles of semantic composition presented above.  The {\it head-complement} rule 
unifies the verb {\it tried}'s constraints on
its complement with those of the VP phrase {\it to bark}, which results in the
identification of the {\sc xarg} value of that VP with the {\sc index} of the 
subject of {\it tried}.  The constraints on {\it tried}'s subject are propagated up
to the verb phrase {\it tried to bark} from the head-daughter {\it tried} by
the head-complement rule, and the semantics of this verb phrase preserve the
semantic properties of its daughters, including the desired re-entrancies with
the subject index.  Hence when the head-subject rule combines {\it the dog}
with {\it tried to bark}, the syntactic and semantic constraints of the noun
phrase are unified with those in the {\sc subj} attribute of the verb phrase,
resulting in the identification of the {\sc arg0} value introduced by {it dog}
with the {\sc arg1} values in both {\bf \_try\_v\_rel} and {\bf \_bark\_v\_rel}.

\subsubsection{Raising}

The same {\sc xarg} attribute enables a straightforward representation of
raising phenomena, where a syntactic argument's semantic index is not linked
to any semantic argument position in a given lexical entry's semantic relation,
but is instead assigned to a role in the semantics of another syntactic 
argument of the lexical entry.  For example, the English verb {\it seem} can
have two syntactic arguments, and NP and a VP, as in {\it the dog seems to
bark}, but the semantics of {\it seem} introduces a one-argument relation
{\bf \_seem\_v\_rel} which takes a proposition.

Compare the following parse tree and lexical type definition to the ones
for the subject-control example with {\it try} above:

\enumsentence{\label{raising}
The dog seems to bark.

\begin{tree}
\psset{levelsep=.7in}
\br{S}{\br{\begin{avm}\[ index & \@1 \]\end{avm}}{
    \br{Det}{\lf{the}}
    \br{\begin{avm}\[ index & \@1\\ arg0 & \@1 \]\end{avm}}{\lf{dog}}}
       \br{\begin{avm}\[ subj & \q< \[ index & \@1 \] \q> \]\end{avm}}{
	 \br{V}{\lf{seems}}
	 \br{\begin{avm}\[ xarg & \@1 \]\end{avm}}{
	   \br{C}{\lf{to}}
	   \br{\begin{avm}\[ xarg & \@1\\ arg1 & \@1 \]\end{avm}}{
	     \lf{bark}}}}}
\end{tree}

%% \begin{tree}
%% \br{S}{\br{$\avmplus{\attval{INDEX}{\ind{1}}}$}
%%            {\br{Det}{\lf{the}}
%%             \br{$\avmplus{\attval{INDEX}{\ind{1}}\\
%%                           \attval{ARG0}{\ind{1}}}$}{\lf{dog}}}
%%        \br{$\avmplus{\attval{S.INDEX}{\ind{1}}}$}
%%            {\br{V}{\lf{seems}}
%%             \br{$\avmplus{\attval{XARG}{\ind{1}}}$}
%%                 {\br{C}{\lf{to}}
%%                  \br{$\avmplus{\attval{XARG}{\ind{1}}\\
%%                                \attval{ARG1}{\ind{1}}}$}{\lf{bark}}}}}
%% \end{tree}\\

}

\enumsentence{\label{srs}
Type for subject-raising verbs like {\it seem}

\begin{avm}
\[ {\it subj-rais-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook & \[ ltop & \@2\\
                            xarg & \@1 \] \] \q>\\
   {-}{-}keyrel & \[ arg1 & \@2 \] \]
\end{avm}

%% $\avmplus{\att{\it subj-rais-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%                       \avmplus{\attval{HOOK} {
%%                                        \avmplus{\attval{LTOP}{\ind{2}}\\
%%                                                  \attval{XARG}{\ind{1}}}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{2}}}}}$
}


The difference between the lexical entries for{\it seem} and {\it try} is
that the raising verb identifies the external argument of its VP complement
with its own subject's index, but then takes the proposition introduced by
the VP complement as its only argument, rather than assigning a second
semantic role to its own subject's index.

Object-raising lexical entries like the English {\it believe} instantiate
a lexical type similar to the one for subject-raising verbs, but identify 
the external argument of their VP complement with the index of their direct
object NP rather than that of their subject.  The relevant type is defined
as follows:

\enumsentence{\label{ors}
Type for object-raising verbs like {\it believe}

\begin{avm}
\[ {\it obj-rais-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.index & \@2 \] , \[ hook & \[ ltop & \@3\\
                                                     xarg & \@2 \]\] \q>\\
   {-}{-}keyrel & \[ arg1 & \@1\\
                     arg2 & \@3 \] \]
\end{avm}

%% $\avmplus{\att{\it obj-rais-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%                       \avmplus{\attval{HOOK.INDEX}{\ind{2}}},
%%                       \avmplus{\attval{HOOK} {
%%                                        \avmplus{\attval{LTOP}{\ind{3}}\\
%%                                                 \attval{XARG}{\ind{2}}}}}}\\
%%           \attvallist{CONT.RELS}{
%%                            \avmplus{\attval{ARG1}{\ind{1}}\\
%%                                     \attval{ARG2}{\ind{3}}}}}$

}
\subsubsection{Expletives}

Constructions with expletive arguments are analyzed similarly to raising
constructions in that a lexical entry's syntactic argument introduces a
semantic index which does not appear as a role in the entry's semantic
predicate; indeed, that index appears in no relation in the semantics.
For example, the English sentence {\it it rained} is analyzed in the ERG
to have the semantics {\bf \_rain\_v\_rel}(e), with the lexical entry for
{\it rain} requiring its subject NP to be the expletive {\it it}.  The
lexical type for verbs like {\it rain} introduces the following constraints,
making use of an English-specific subtype of the Matrix {\it expl-ind} named
{\it it-ind} which is introduced by the relevant lexical entry for the NP
{\it it}.

\es{
\begin{avm}
\[ {\it expl-subj-verb}\\
   subj & \q< \[ hook.index & {\it it-ind} \]\\
   {-}{-}keyrel & \[ arg0 & event \] \]
\end{avm}
}

%% $\avmplus{\att{\it expl-subj-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\it it-ind}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG0}{\it event}}}}$

The lexical entry for the English presentational {\it be} as in the example
{\it there is a dog here} requires its subject NP to have a semantic index
of a second type of expletive, called {\it there-ind}, which is introduced
by the relevant lexical entry for the NP {\it there}.  The following example
illustrates the interaction of expletives with object-raising predicates:

\enumsentence{\label{expl-rais}
The dog believes there to be a cat
}


\section{Generation}
\label{gen}

%[ ... What you need to do to make sure you can generate from your
%grammar, info on writing filters for semantically empty lexical items, ... ] 

\section{Verifying {\it mrs}s}
\label{verif}

%[... describe the scoping machinery, the different ways of viewing
%{\it mrs}s in the LKB, and the color coding of MRS ``health'' in the
%treebanking tool ...]

\section{Sem-I: The Semantic Interface}
\label{semisec}

In order to make use of a semantic representation produced by a Matrix
grammar, an application developer will require a specification of how
to interpret the MRS structures, including documentation of the full
range of semantic relations and their arguments that the grammar introduces.
This specification, called the Semantic Interface (Sem-I) should be supplied
as part of each grammar, and will be the locus of some linguistic 
generalizations that one might have expected to see encoded directly in the
lexical type hierarchy or in lexical rules.

The Sem-I will consist of:
\begin{enumerate}
\item A manually-constructed
fully-documented database of all the relations introduced 
via constructions
and lexical types, and all the values which may appear on semantic 
features.  This is the meta-level Sem-I, and should be consistent across
languages: i.e., it forms part of the Matrix.
\item An automatically constructed database for the semantic information
pertaining to the open class words in each language.
This is the object-level Sem-I.  It is generated from the lexical
database for each grammar's lexicon.
\end{enumerate}

The Matrix will have associated code for generating the object-level
Sem-I, for validating the well-formedness of {\it mrs}s constructed by the 
grammar, and for employing Sem-I mappings in displaying MRS representations.

\subsection{The meta-level Sem-I}

This is a manually constructed,  fully documented database of all the 
relations introduced via constructions and lexical types, and all the values 
which may appear on semantic features.  Since this inventory is intended
to be consistent across languages, it forms part of the Matrix,
with links to language-specific examples that illustrate the use of these
relations and values for a particular grammar.

For example, the two-place relation introduced for noun-noun compounds in
the Matrix will appear in the meta-level Sem-I database as follows:\\
\begin{tabular}{lllllllll}
PRED & ARG0 & & ARG1 & & ARG2 & & documentation & test suite eg\\
unspec\_compound\_rel & event & oblig & 
index & oblig & index & oblig & \verb+< link to documentation >+ & 
\verb+< eg num >+
\end{tabular}

Here, the specification and instantiation of the types
of the ARG0, ARG1, ARG2 values may be done automatically, as will be the case
with the object-level Sem-I.  Each semantic role is identified as an
obligatory ('oblig') or optional ('opt') argument of the predicate, with the 
expectation that more fine-grained distinctions may be needed later.
The documentation needs to explain the meaning and use
of the relation in as much detail as necessary for application
developers to decide how to treat it.  For instance, it would be 
important for a developer
to know whether unspec\_compound\_rel was used for all 
noun-noun compounds in this grammar or only for some of them.

The meta-level Sem-I will also include specifications of semantic classes
to provide the basis for generalizations over thematic roles, and to enable
more mnemonic displays of MRS structures where the role names can be
specialized for these semantic classes.

\subsection{The object-level Sem-I}

Each grammar built using the Matrix should include an automatically 
constructed database for the semantic information pertaining to the open class 
words in the language.  This is the object-level Sem-I, generated from the 
lexical database for each lexicon, with possible links to example sentences
that can be used for testing.

For example:\\

\noindent
{\small
\begin{tabular}{lllllllllll}
lexeme & string & PRED & ARG0 & & ARG1 & & ARG2 & & test suite eg & doc\\
chase\_v1 & chase & \_chase\_v\_rel & event & $+$ & index & $+$ & index & $+$ & Dogs chased cats & ``Doc''
\end{tabular}
}\\

In most cases there won't be a test suite example, but it may be that a
developer will add one to elucidate the use or distinction from another sense. 
Similarly, documentation may be added, but usually won't be.
The mechanism for adding documentation or test suite examples
must make it possible to autogenerate the links in this database.


\subsection{Thematic role mapping}

The Matrix uses a naming convention where ARGn is used to identify
individual semantic arguments for predicates.  While this has proven to be
important in capturing generalizations about linking in the lexical type
hierarchy, it requires that other linguistic generalizations over thematic
roles be expressed instead in the Sem-I.

The approach employed in the Matrix is two-fold: a rich inventory
of word classes will be incrementally developed which will allow mapping
of ARGn relations into alternative inventories, and since this is
a long term project, it might be augmented with automatically
generated example sentences as a form of cheap documentation to convey the
intent of role assignments for any given predicate.

\subsection{Word classes}

Information about word classes should be added
incrementally to the lexical databases, without changing the grammars.
Documentation for these classes explains the notion of ARGn wrt that class, and
mappings to alternative thematic roles can be done on the basis of class 
membership.
Verb classes will be part of the Matrix Sem-I, since they are motivated by 
properties of the syntax-semantics interface.  In contrast, thematic role 
mappings are additions to the Sem-I which may be provided by developers of
a particular grammar.  The LKB software will support parameterized I/O routines
which will allow alternative thematic
role inventories to be supported if there is an available mapping.

For example, we can distinguish two classes of
English psych predicates: one in which the subject is
the experiencer of some emotion and the object the stimulus
(e.g., {\it Kim likes rabbits}) and the other in which the converse is true
(e.g. {\it Rabbits worry Kim}).  The lexical database entries
for {\it like} and {\it worry} can be enhanced to include their class.

A grammar developer wishing to add a semantic class to the Sem-I
should provide the following information:
\begin{enumerate}
\item Class name.
\item Class documentation, to include 
class membership criteria in the form of specific tests
and some specific exemplars.
\item Documentation of the ARGn behaviour of the class in the grammar.
\item A full list of all current lexical entries that are members
of the class.
\item A mapping between the ARGn for this class and any supported
thematic role sets.
\end{enumerate}

\subsection{Example sentences}
\label{sempref}

Automatically created examples might be used to
make the behaviour of lexical entries clearer to outside users.
The idea is to augment entries which have subcategorization with
some very coarse-grained selectional restriction information.  For instance:
\begin{itemize}
\item handle
\item event
\item non referential
\item referential
\begin{enumerate}
\item animate
\item non-animate physical
\item physical location
\item temporal location
\item abstract
\end{enumerate}
\end{itemize}
This would allow us to automatically construct standardized examples,
which can be used to illustrate ARGs etc.
For instance:
\begin{quote}
the person liked the thing\\
the person liked doing the thing\\
the person liked to do the thing
\end{quote}
where {\it person} is the standard term for animate entities and 
{\it thing} for non-animate ones.

The point about this approach is to make it reasonably intuitive for outsiders
without the process of deciding on the semantic categories becoming too
time-consuming.  It would also make it reasonably easy to check the
entries for over-generation, by 
scanning the automatically generated examples for obviously incorrect
sentences.

\section{Conclusion}

\section{Acknowledgments}

This paper is an extension of \citeboth{Fli:Ben:03}.

\ldots

\bibliographystyle{robbib}
\bibliography{./userguide}

\section*{Index}

\end{document}

